{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 12:55:01 - ast_utils - DEBUG    - Using cache folder: /Users/guydavidson/tmp/game_generation_cache\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import duckdb\n",
    "from functools import reduce\n",
    "import glob\n",
    "import gzip\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import tatsu, tatsu.ast\n",
    "import tqdm.notebook as tqdmn\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('../reward-machine'))\n",
    "\n",
    "import compile_predicate_statistics\n",
    "import compile_predicate_statistics_split_args\n",
    "from compile_predicate_statistics_split_args import *\n",
    "from config import SPECIFIC_NAMED_OBJECTS_BY_ROOM\n",
    "import config\n",
    "from utils import get_object_assignments\n",
    "\n",
    "import ast_printer\n",
    "import ast_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = compile_predicate_statistics.get_project_dir() + '/reward-machine/caches'\n",
    "\n",
    "no_strings_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics_no_intervals_028b3733.pkl.gz'))\n",
    "with gzip.open(os.path.join(cache_dir, 'trace_lengths_7511b7be.pkl'), 'rb') as f:\n",
    "    trace_lengths_and_domains = pickle.load(f)\n",
    "\n",
    "\n",
    "# regular_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics.pkl'))\n",
    "# split_args_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics_028b3733.pkl.gz'))\n",
    "# split_args_df = split_args_df[split_args_df['predicate'] != 'same_type']\n",
    "# print(split_args_df.shape)\n",
    "# # split_args_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics_4d5dd602.pkl.gz'))\n",
    "\n",
    "# # stats = compile_predicate_statistics.CommonSensePredicateStatistics(cache_dir)\n",
    "# split_args_stats = compile_predicate_statistics_split_args.CommonSensePredicateStatisticsSplitArgs(\n",
    "#     # cache_dir, compile_predicate_statistics_split_args.CURRENT_TEST_TRACE_NAMES, overwrite=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_df_no_same_type = split_args_df[split_args_df['predicate'] != 'same_type'].drop(columns=['string_intervals'])\n",
    "split_args_df_no_same_type.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_df_no_same_type.to_pickle(os.path.join(cache_dir, 'predicate_statistics_028b3733.pkl.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_stats.trace_lengths_and_domains_df.select(pl.col('trace_length').max()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(10000, dtype=np.uint8)\n",
    "a[100:1000] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros(10, dtype=np.uint8)\n",
    "t[1:4] = 1\n",
    "b = t.tobytes()\n",
    "d = {b[0]: '0', b[1]: '1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(map(lambda x: d[x], b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bytes([0, 1])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = split_args_stats.trace_lengths_and_domains_df.select(pl.col('trace_length').max()).item()\n",
    "print(MAX_LENGTH)\n",
    "b = bytes([0, 1])\n",
    "BYTE_MAPPING = {b: str(i) for i, b in enumerate(b)}\n",
    "\n",
    "\n",
    "\n",
    "def intervals_to_string(intervals, max_length: int = MAX_LENGTH):\n",
    "    value = np.zeros(max_length, dtype=np.uint8)\n",
    "    for interval in intervals:\n",
    "        value[interval[0]:interval[1]] = 1\n",
    "\n",
    "    return ''.join(map(lambda b: BYTE_MAPPING[b], value.tobytes()))\n",
    "    # return np.array2string(value, separator='', threshold=max_length + 10)[1:-1].replace('\\n ', '')\n",
    "\n",
    "\n",
    "intervals = split_args_df.intervals.apply(intervals_to_string)\n",
    "\n",
    "# small_split_args_df = small_split_args_df.assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_df = split_args_df.assign(string_intervals=intervals)\n",
    "split_args_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('DROP TABLE data')\n",
    "duckdb.sql('CREATE TABLE data AS SELECT * FROM split_args_df')\n",
    "duckdb.sql('ALTER TABLE data ALTER string_intervals TYPE BITSTRING')\n",
    "duckdb.sql('DESCRIBE data').show()\n",
    "\n",
    "# duckdb.sql(\"CREATE TYPE predicate AS ENUM (SELECT predicate FROM split_args_df);\")\n",
    "# duckdb.sql(\"CREATE TYPE domain AS ENUM (SELECT domain FROM split_args_df);\")\n",
    "# duckdb.sql(\"CREATE TYPE trace_id AS ENUM (SELECT trace_id FROM split_args_df);\")\n",
    "# all_types = tuple([t for t in set(split_args_df.arg_1_type.unique()) | set(split_args_df.arg_2_type.unique()) if isinstance(t, str) ])\n",
    "# duckdb.sql(f\"CREATE TYPE arg_type AS ENUM {all_types};\")\n",
    "# all_ids = tuple([t for t in set(split_args_df.arg_1_id.unique()) | set(split_args_df.arg_2_id.unique()) if isinstance(t, str)])\n",
    "# duckdb.sql(f\"CREATE TYPE arg_id AS ENUM {all_ids};\")\n",
    "\n",
    "# duckdb.sql('ALTER TABLE data ALTER predicate TYPE predicate')\n",
    "# duckdb.sql('ALTER TABLE data ALTER domain TYPE domain')\n",
    "# duckdb.sql('ALTER TABLE data ALTER trace_id TYPE trace_id')\n",
    "# duckdb.sql('ALTER TABLE data ALTER arg_1_type TYPE arg_type')\n",
    "# duckdb.sql('ALTER TABLE data ALTER arg_2_type TYPE arg_type')\n",
    "# duckdb.sql('ALTER TABLE data ALTER arg_1_id TYPE arg_id')\n",
    "# duckdb.sql('ALTER TABLE data ALTER arg_2_id TYPE arg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tld_df = split_args_stats.trace_lengths_and_domains_df\n",
    "duckdb.sql('DROP TABLE trace_lengths_domains')\n",
    "duckdb.sql('CREATE TABLE trace_lengths_domains AS SELECT * FROM tld_df')\n",
    "duckdb.sql('ALTER TABLE trace_lengths_domains ALTER domain TYPE domain')\n",
    "duckdb.sql('ALTER TABLE trace_lengths_domains ALTER trace_id TYPE trace_id')\n",
    "duckdb.sql('SELECT * FROM trace_lengths_domains').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT arg_1_type FROM data WHERE arg_1_type='ball'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT arg_1_id as '?b' FROM data WHERE arg_1_type = 'dodgeball' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_GRAMMAR_PATH = \"../dsl/dsl.ebnf\"\n",
    "grammar = open(DEFAULT_GRAMMAR_PATH).read()\n",
    "grammar_parser = typing.cast(tatsu.grammars.Grammar, tatsu.compile(grammar))\n",
    "\n",
    "game = open(get_project_dir() + '/reward-machine/games/ball_to_bin_from_bed.txt').read()\n",
    "game_ast = grammar_parser.parse(game)  # type: ignore\n",
    "\n",
    "block_stacking_game = open(get_project_dir() + '/reward-machine/games/block_stacking.txt').read()\n",
    "block_stacking_game_ast = grammar_parser.parse(block_stacking_game)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred_desk_and = block_stacking_game_ast[3][1]['preferences'][1]['definition']['pref_body']['body']['exists_args']['at_end_pred']\n",
    "# ast_printer.ast_section_to_string(test_pred_desk_and, ast_parser.PREFERENCES)\n",
    "\n",
    "test_pred_or = block_stacking_game_ast[3][1]['preferences'][2]['definition']['pref_body']['body']['exists_args']['at_end_pred']\n",
    "print(ast_printer.ast_section_to_string(test_pred_or, ast_parser.PREFERENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspsd.filter(test_pred_or, {\"?b\": [\"block\"], \"?c\": [\"chair\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"SELECT temp_table_3.trace_id, temp_table_3.domain, temp_table_3.\"top_drawer\", temp_table_3.\"?g\", \n",
    "(temp_table_3.string_intervals | COALESCE(temp_table_1.string_intervals, empty_bitstring()) | COALESCE(temp_table_2.string_intervals, empty_bitstring())) AS si,\n",
    "FROM temp_table_3 \n",
    "LEFT JOIN temp_table_1 ON temp_table_3.trace_id=temp_table_1.trace_id AND temp_table_1.\"top_drawer\"=temp_table_3.\"top_drawer\" AND temp_table_1.\"?g\"=temp_table_3.\"?g\" \n",
    "LEFT JOIN temp_table_2 ON temp_table_3.trace_id=temp_table_2.trace_id AND temp_table_2.\"top_drawer\"=temp_table_3.\"top_drawer\" \n",
    "\"\"\"\n",
    "\n",
    "# temp_table_3.string_intervals as si3, COALESCE(temp_table_1.string_intervals, empty_bitstring()) as si1, COALESCE(temp_table_2.string_intervals, empty_bitstring()) AS si2,\n",
    "# \n",
    "q2 = \"\"\"SELECT temp_table_3.trace_id, temp_table_3.domain, temp_table_3.\"top_drawer\", temp_table_3.\"?g\", \n",
    "(temp_table_3.string_intervals | COALESCE(temp_table_1.string_intervals, empty_bitstring()) | COALESCE(temp_table_2.string_intervals, empty_bitstring())) AS si,\n",
    "FROM temp_table_3 \n",
    "LEFT JOIN temp_table_1 ON temp_table_3.trace_id=temp_table_1.trace_id AND temp_table_1.\"top_drawer\"=temp_table_3.\"top_drawer\" AND temp_table_1.\"?g\"=temp_table_3.\"?g\" \n",
    "LEFT JOIN temp_table_2 ON temp_table_3.trace_id=temp_table_2.trace_id AND temp_table_2.\"top_drawer\"=temp_table_3.\"top_drawer\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pldf = pl.from_dataframe(split_args_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckdb.sql(\"CREATE TYPE predicate AS ENUM (SELECT predicate FROM split_args_df);\")\n",
    "# duckdb.sql(\"CREATE TYPE domain AS ENUM (SELECT domain FROM split_args_df);\")\n",
    "# duckdb.sql(\"CREATE TYPE trace_id AS ENUM (SELECT trace_id FROM split_args_df);\")\n",
    "# all_types = tuple([t for t in set(split_args_df.arg_1_type.unique()) | set(split_args_df.arg_2_type.unique()) if isinstance(t, str) ])\n",
    "# duckdb.sql(f\"CREATE TYPE arg_type AS ENUM {all_types};\")\n",
    "# all_ids = tuple([t for t in set(split_args_df.arg_1_id.unique()) | set(split_args_df.arg_2_id.unique()) if isinstance(t, str)])\n",
    "# duckdb.sql(f\"CREATE TYPE arg_id AS ENUM {all_ids};\")\n",
    "\n",
    "# duckdb.sql('ALTER TABLE test ALTER predicate TYPE predicate')\n",
    "# duckdb.sql('ALTER TABLE test ALTER domain TYPE domain')\n",
    "# duckdb.sql('ALTER TABLE test ALTER trace_id TYPE trace_id')\n",
    "# duckdb.sql('ALTER TABLE test ALTER arg_1_type TYPE arg_type')\n",
    "# duckdb.sql('ALTER TABLE test ALTER arg_2_type TYPE arg_type')\n",
    "# duckdb.sql('ALTER TABLE test ALTER arg_1_id TYPE arg_id')\n",
    "# duckdb.sql('ALTER TABLE test ALTER arg_2_id TYPE arg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('DESCRIBE test').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT bit_count((SELECT string_intervals FROM test WHERE predicate='on' LIMIT 1) & (SELECT string_intervals FROM test WHERE predicate='on' OFFSET 100 LIMIT 1))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT intervals FROM test WHERE predicate='on' LIMIT 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_type = 'basketball'\n",
    "duckdb.sql(f\"SELECT count(*) FROM data WHERE (arg_1_type='{arg_type}' OR arg_2_type='{arg_type}')\").fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SELECT val FROM (SELECT unnest(enum_range(NULL::arg_id)) as val) \")  # WHERE val='Shelf|-02.97|+01.53|-01.72' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_df.select(pl.col('bits_1') & pl.col('bits_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_specific_names = set()\n",
    "for room, type_to_ids in SPECIFIC_NAMED_OBJECTS_BY_ROOM.items():\n",
    "    all_specific_names.update(type_to_ids.keys())\n",
    "\n",
    "specific_name_rows = split_args_df[(split_args_df.arg_1_type.isin(all_specific_names) | split_args_df.arg_2_type.isin(all_specific_names))].shape[0]\n",
    "total_rows = split_args_df.shape[0]\n",
    "print(f'{specific_name_rows} / {total_rows} ({specific_name_rows / total_rows * 100:.2f}%) rows have a specific name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_type_rows = split_args_df[(split_args_df.predicate == 'same_type')].shape[0]\n",
    "print(f'{same_type_rows} / {total_rows} ({same_type_rows / total_rows * 100:.2f}%) rows are for same_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in split_args_df.iterrows():\n",
    "    print(i, row.to_dict())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_df[(split_args_df.predicate == 'adjacent') & (split_args_df.arg_1_type == 'agent') & (split_args_df.arg_2_type == 'green_golfball')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_df[(split_args_df.predicate == 'same_type')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expriment with moving everything to a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_types = set([t for t in set(no_strings_df.arg_1_type.unique()) | set(no_strings_df.arg_2_type.unique()) if isinstance(t, str)])\n",
    "all_predicates = set([t for t in set(no_strings_df.predicate.unique()) if isinstance(t, str)])\n",
    "all_arg_ids = set([t for t in set(no_strings_df.arg_1_id.unique()) | set(no_strings_df.arg_2_id.unique()) if isinstance(t, str)])\n",
    "all_arg_ids.update(config.UNITY_PSEUDO_OBJECTS.keys())\n",
    "all_arg_ids.update(reduce(lambda x, y: x + y, [object_types for room_types in config.OBJECTS_BY_ROOM_AND_TYPE.values() for object_types in room_types.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_types = set(reduce(lambda x, y: x + y, [list(x.keys()) for x in itertools.chain(config.OBJECTS_BY_ROOM_AND_TYPE.values(), config.SPECIFIC_NAMED_OBJECTS_BY_ROOM.values())]))\n",
    "# config_types.difference_update(config.META_TYPES.keys())\n",
    "config_types.remove(config.GAME_OBJECT)\n",
    "\n",
    "config_predicates = set([t[0] for t in compile_predicate_statistics_split_args.COMMON_SENSE_PREDICATES_AND_FUNCTIONS])\n",
    "\n",
    "config_arg_ids = set(reduce(lambda x, y: x + y, [object_types for room_types in config.OBJECTS_BY_ROOM_AND_TYPE.values() for object_types in room_types.values()]))\n",
    "config_arg_ids.update(config.UNITY_PSEUDO_OBJECTS.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for enum_name in ('domain', 'trace_id', 'predicate', 'arg_type', 'arg_id'):\n",
    "    duckdb.sql(f'DROP TYPE {enum_name} CASCADE;')\n",
    "\n",
    "duckdb.sql(f\"CREATE TYPE domain AS ENUM {tuple(config.ROOMS)};\")\n",
    "all_trace_ids = [os.path.splitext(os.path.basename(p))[0] for p in glob.glob('../reward-machine/traces/participant-traces/*.json')]\n",
    "duckdb.sql(f\"CREATE TYPE trace_id AS ENUM {tuple(all_trace_ids)};\")\n",
    "duckdb.sql(f\"CREATE TYPE predicate AS ENUM {tuple(sorted(config_predicates))};\")\n",
    "duckdb.sql(f\"CREATE TYPE arg_type AS ENUM {tuple(sorted(config_types))};\")\n",
    "duckdb.sql(f\"CREATE TYPE arg_id AS ENUM {tuple(sorted(config_arg_ids))};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace lengths and domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('DROP TABLE IF EXISTS trace_length_and_domains;')\n",
    "duckdb.sql('CREATE TABLE trace_length_and_domains(trace_id trace_id, domain domain, length INTEGER);')\n",
    "trace_length_and_domain_rows = [(trace_id, domain, length) for (trace_id, (length, domain)) in trace_lengths_and_domains.items()]\n",
    "duckdb.sql(f'INSERT INTO trace_length_and_domains VALUES {str(tuple(trace_length_and_domain_rows))[1:-1]}')\n",
    "\n",
    "duckdb.sql('DROP TABLE IF EXISTS empty_bitstrings;')\n",
    "duckdb.sql(\"CREATE TABLE empty_bitstrings AS (SELECT trace_id, BITSTRING('0', length) as intervals FROM trace_length_and_domains)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('DROP TABLE IF EXISTS meta_types;')\n",
    "duckdb.sql('CREATE TABLE meta_types(meta_type arg_type, type arg_type);')\n",
    "for meta_type, meta_type_sub_types in config.META_TYPES.items():\n",
    "    for sub_type in meta_type_sub_types:\n",
    "        duckdb.sql(f'INSERT INTO meta_types VALUES (\\'{meta_type}\\', \\'{sub_type}\\');')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('DROP TABLE IF EXISTS object_type_to_id;')\n",
    "duckdb.sql('CREATE TABLE object_type_to_id(domain domain, type arg_type, object_id arg_id);')\n",
    "\n",
    "data_rows = []\n",
    "for domain in config.ROOMS:\n",
    "    for object_dict in (config.OBJECTS_BY_ROOM_AND_TYPE[domain], config.SPECIFIC_NAMED_OBJECTS_BY_ROOM[domain]):\n",
    "        for object_type, object_ids in object_dict.items():\n",
    "            if object_type in config_types:\n",
    "                for object_id in object_ids:\n",
    "                    data_rows.append((domain, object_type, object_id))\n",
    "\n",
    "\n",
    "duckdb.sql(f'INSERT INTO object_type_to_id VALUES {str(tuple(data_rows))[1:-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('SELECT intervals | NULL from empty_bitstrings LIMIT 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cachetools\n",
    "# import operator\n",
    "\n",
    "# from config import GAME_OBJECT, GAME_OBJECT_EXCLUDED_TYPES, META_TYPES\n",
    "\n",
    "# from ast_parser import PREFERENCES\n",
    "# from ast_printer import ast_section_to_string\n",
    "\n",
    "\n",
    "# DEBUG = True\n",
    "\n",
    "# class PredicateNotImplementedException(Exception):\n",
    "#     pass\n",
    "\n",
    "\n",
    "# class MissingVariableException(Exception):\n",
    "#     pass\n",
    "\n",
    "\n",
    "# class CommonSensePredicateStatisticsFullDatabse():\n",
    "\n",
    "#     def __init__(self): \n",
    "#         self.predicates = config_predicates\n",
    "#         self.cache = cachetools.LRUCache(maxsize=10000)\n",
    "#         self.temp_table_index = -1\n",
    "#         self.temp_table_prefix = 't'\n",
    "\n",
    "#     def _table_name(self, index: int):\n",
    "#         return f\"{self.temp_table_prefix}{index}\"\n",
    "\n",
    "#     def _next_temp_table_index(self):\n",
    "#         self.temp_table_index += 1\n",
    "#         return self.temp_table_index\n",
    "\n",
    "#     def _next_temp_table_name(self):\n",
    "#         return self._table_name(self._next_temp_table_index())\n",
    "\n",
    "#     def filter(self, predicate: tatsu.ast.AST, mapping: typing.Dict[str, typing.Union[str, typing.List[str]]], **kwargs):\n",
    "#         try:\n",
    "#             self.temp_table_index = -1\n",
    "#             result_query, _ = self._inner_filter(predicate, mapping, **kwargs)\n",
    "#             return result_query\n",
    "        \n",
    "#         except PredicateNotImplementedException as e:\n",
    "#             # Pass the exception through and let the caller handle it\n",
    "#             raise e\n",
    "\n",
    "#     def _predicate_and_mapping_cache_key(self, predicate: tatsu.ast.AST, mapping: typing.Dict[str, typing.Union[str, typing.List[str]]], *args, **kwargs) -> str:\n",
    "#         '''\n",
    "#         Returns a string that uniquely identifies the predicate and mapping\n",
    "#         '''\n",
    "#         return ast_section_to_string(predicate, PREFERENCES) + \"_\" + str(mapping)\n",
    "\n",
    "#     @cachetools.cachedmethod(operator.attrgetter('cache'), key=_predicate_and_mapping_cache_key)\n",
    "#     def _handle_predicate(self, predicate: tatsu.ast.AST, mapping: typing.Dict[str, typing.Union[str, typing.List[str]]], return_trace_ids: bool = False, **kwargs) -> typing.Tuple[str, typing.Set[str]]:\n",
    "#         predicate_name = extract_predicate_function_name(predicate)  # type: ignore\n",
    "\n",
    "#         if predicate_name not in self.predicates:\n",
    "#             raise PredicateNotImplementedException(predicate_name)\n",
    "\n",
    "#         variables = extract_variables(predicate)  # type: ignore\n",
    "#         used_variables = set(variables)\n",
    "\n",
    "#         # Restrict the mapping to just the referenced variables and expand meta-types\n",
    "#         relevant_arg_mapping = {}\n",
    "#         for var in variables:\n",
    "#             if var in mapping:\n",
    "#                 relevant_arg_mapping[var] = sum([META_TYPES.get(arg_type, [arg_type]) for arg_type in mapping[var]], [])\n",
    "\n",
    "#             # This handles variables which are referenced directly, like the desk and bed\n",
    "#             elif not var.startswith(\"?\"):\n",
    "#                 relevant_arg_mapping[var] = [var]\n",
    "\n",
    "#             else:\n",
    "#                 raise MissingVariableException(f\"Variable {var} is not in the mapping\")\n",
    "\n",
    "#         select_items = [\"trace_id\", \"domain\", \"string_intervals\"]\n",
    "#         where_items = [f\"predicate='{predicate_name}'\"]\n",
    "\n",
    "#         for i, (arg_var, arg_types) in enumerate(relevant_arg_mapping.items()):\n",
    "#             # if it can be the generic object type, we filter for it specifically\n",
    "#             if GAME_OBJECT in arg_types:\n",
    "#                 where_items.append(f\"arg_{i + 1}_type NOT IN {tuple(GAME_OBJECT_EXCLUDED_TYPES)}\")\n",
    "\n",
    "#             else:\n",
    "#                 if len(arg_types) == 1:\n",
    "#                     where_items.append(f\"arg_{i + 1}_type='{arg_types[0]}'\")\n",
    "#                 else:\n",
    "#                     where_items.append(f\"arg_{i + 1}_type IN {tuple(arg_types)}\")\n",
    "\n",
    "#             select_items.append(f\"arg_{i + 1}_id as '{arg_var}'\")\n",
    "\n",
    "#         query = f\"SELECT {select_items} FROM data WHERE {' AND '.join(where_items)};\"\n",
    "#         return query, used_variables\n",
    "\n",
    "#     @cachetools.cachedmethod(operator.attrgetter('cache'), key=_predicate_and_mapping_cache_key)\n",
    "#     def _handle_and(self, predicate: tatsu.ast.AST, mapping: typing.Dict[str, typing.Union[str, typing.List[str]]], **kwargs) -> typing.Tuple[str, typing.Set[str]]:\n",
    "#         and_args = predicate[\"and_args\"]\n",
    "#         if not isinstance(and_args, list):\n",
    "#             and_args = [and_args]\n",
    "\n",
    "#         sub_queries = []\n",
    "#         used_variables_by_child = []\n",
    "#         all_used_variables = set()\n",
    "\n",
    "#         for and_arg in and_args:  # type: ignore\n",
    "#             try:\n",
    "#                 sub_query, sub_used_variables = self._inner_filter(and_arg, mapping)  # type: ignore\n",
    "#                 sub_queries.append(sub_query)\n",
    "#                 used_variables_by_child.append(sub_used_variables)\n",
    "#                 all_used_variables |= sub_used_variables\n",
    "\n",
    "#             except PredicateNotImplementedException as e:\n",
    "#                 continue\n",
    "\n",
    "#         if len(sub_queries) == 0:\n",
    "#             raise PredicateNotImplementedException(\"All sub-predicates of the and were not implemented\")\n",
    "\n",
    "#         if len(sub_queries) == 1:\n",
    "#             return sub_queries[0], used_variables_by_child[0]\n",
    "        \n",
    "#         subquery_table_names = [self._next_temp_table_name() for _ in range(len(sub_queries))]\n",
    "\n",
    "#         select_items = [f\"{subquery_table_names[0]}.trace_id\", f\"{subquery_table_names[0]}.domain\"]\n",
    "#         selected_variables = set()\n",
    "#         intervals = []\n",
    "#         join_clauses = []\n",
    "\n",
    "#         for i, (sub_query, table_name, sub_used_variables) in enumerate(zip(sub_queries, subquery_table_names, used_variables_by_child)):\n",
    "#             intervals.append(f\"{sub_query}.intervals\")\n",
    "\n",
    "#             for variable in sub_used_variables:\n",
    "#                 if variable not in selected_variables:\n",
    "#                     select_items.append(f'{sub_query}.\"{variable}\"')\n",
    "#                     selected_variables.add(variable)\n",
    "\n",
    "#             if i > 0:\n",
    "#                 join_parts = [f\"INNER JOIN ({sub_query}) AS {table_name} ON ({subquery_table_names[0]}.trace_id={table_name}.trace_id)\"]\n",
    "\n",
    "#                 for j, (prev_table_name, prev_used_variables) in enumerate(zip(subquery_table_names[:i], used_variables_by_child[:i])):\n",
    "#                     shared_variables = sub_used_variables & prev_used_variables\n",
    "#                     join_parts.extend([f'({table_name}.\"{v}\"={prev_table_name}.\"{v}\")' for v in shared_variables])\n",
    "\n",
    "#                 join_clauses.append(\" AND \".join(join_parts))\n",
    "\n",
    "\n",
    "#         select_items.append(f'({\" & \".join(intervals)}) AS intervals')\n",
    "\n",
    "#         table_name = self._next_temp_table_name()\n",
    "#         query = f\"WITH TABLE {table_name} AS (SELECT {', '.join(select_items)} FROM {subquery_table_names[0]} {' '.join(join_clauses)}) SELECT * FROM {table_name} WHERE bit_count(intervals) != 0;\"\n",
    "#         if DEBUG: print(query)\n",
    "#         return query, all_used_variables\n",
    "\n",
    "\n",
    "#     @cachetools.cachedmethod(operator.attrgetter('cache'), key=_predicate_and_mapping_cache_key)\n",
    "#     def _handle_or(self, predicate: tatsu.ast.AST, mapping: typing.Dict[str, typing.Union[str, typing.List[str]]], **kwargs) -> typing.Tuple[str, typing.Set[str]]:\n",
    "#         or_args = predicate[\"or_args\"]\n",
    "#         if not isinstance(or_args, list):\n",
    "#             or_args = [or_args]\n",
    "\n",
    "#         sub_queries = []\n",
    "#         used_variables_by_child = []\n",
    "#         all_used_variables = set()\n",
    "\n",
    "#         for or_arg in or_args:  # type: ignore\n",
    "#             try:\n",
    "#                 subquery, sub_used_variables = self._inner_filter(or_arg, mapping)  # type: ignore\n",
    "#                 sub_queries.append(subquery)\n",
    "#                 used_variables_by_child.append(sub_used_variables)\n",
    "#                 all_used_variables |= sub_used_variables\n",
    "\n",
    "#             except PredicateNotImplementedException as e:\n",
    "#                 continue\n",
    "\n",
    "#         if len(sub_queries) == 0:\n",
    "#             raise PredicateNotImplementedException(\"All sub-predicates of the por were not implemented\")\n",
    "\n",
    "#         if len(sub_queries) == 1:\n",
    "#             return sub_queries[0], used_variables_by_child[0]\n",
    "\n",
    "#         sub_queries.insert(0, self._build_potential_missing_values_query(mapping, list(all_used_variables)))\n",
    "#         used_variables_by_child.insert(0, all_used_variables)\n",
    "\n",
    "#         subquery_table_names = [self._next_temp_table_name() for _ in range(len(sub_queries))]\n",
    "\n",
    "#         select_items = [f\"{subquery_table_names[0]}.trace_id\", f\"{subquery_table_names[0]}.domain\"]\n",
    "#         selected_variables = set()\n",
    "#         intervals = []\n",
    "#         join_clauses = []\n",
    "\n",
    "#         for i, (subquery, sub_table_name, sub_used_variables) in enumerate(zip(sub_queries, subquery_table_names, used_variables_by_child)):\n",
    "#             intervals.append(f\"{sub_table_name}.intervals\")\n",
    "\n",
    "#             for variable in sub_used_variables:\n",
    "#                 if variable not in selected_variables:\n",
    "#                     select_items.append(f'{sub_table_name}.\"{variable}\"')\n",
    "#                     selected_variables.add(variable)\n",
    "\n",
    "#             if i > 0:\n",
    "#                 join_parts = [f\"LEFT JOIN {sub_table_name} ON ({subquery_table_names[0]}.trace_id={sub_table_name}.trace_id)\"]\n",
    "\n",
    "#                 shared_variables = sub_used_variables & all_used_variables\n",
    "#                 join_parts.extend([f'({subquery}.\"{v}\"={subquery_table_names[0]}.\"{v}\")' for v in shared_variables])\n",
    "\n",
    "#                 join_clauses.append(\" AND \".join(join_parts))\n",
    "\n",
    "#         intervals_coalesce = [f\"COALESCE({intervals_select}, {intervals[0]})\" if i > 0 else intervals_select for i, intervals_select in enumerate(intervals)]\n",
    "#         select_items.append(f'({\" | \".join(intervals_coalesce)}) AS intervals')\n",
    "\n",
    "#         table_name = self._next_temp_table_name()\n",
    "#         query = f\"WITH TABLE {table_name} AS (SELECT {', '.join(select_items)} FROM {subquery_table_names[0]} {' '.join(join_clauses)}) SELECT * FROM {table_name} WHERE bit_count(intervals) != 0;\"\n",
    "#         if DEBUG: print(query)\n",
    "#         return query, all_used_variables\n",
    "\n",
    "#     def _build_object_assignment_cte(self, object_types: typing.Union[str, typing.List[str]]):\n",
    "#         if isinstance(object_types, str) or len(object_types) == 1:\n",
    "#             where_clause = f\"type = '{object_types[0]}'\"\n",
    "#         else:\n",
    "#             where_clause = f\"type IN IN {tuple(object_types)}\"\n",
    "#         return f\"SELECT domain, object_id FROM object_type_to_id WHERE {where_clause}\"\n",
    "\n",
    "#     def object_assignments_query(self, mapping: typing.Dict[str, typing.Union[str, typing.List[str]]]):\n",
    "#         if len(mapping) == 0:\n",
    "#             return []\n",
    "        \n",
    "#         if len(mapping) == 1:\n",
    "#             query = self._build_object_assignment_cte(mapping[list(mapping.keys())[0]])\n",
    "\n",
    "#         else:\n",
    "#             object_id_selects = []\n",
    "#             ctes = []\n",
    "#             join_statements = []\n",
    "#             for i, (var, var_types) in enumerate(mapping.items()):\n",
    "#                 ctes.append(f\"t{i} AS ({self._build_object_assignment_cte(var_types)})\")\n",
    "#                 object_id_selects.append(f\"t{i}.object_id AS '{var}'\")\n",
    "#                 if i > 0:\n",
    "#                     join_clauses = []\n",
    "#                     join_clauses.append(f\"(t0.domain = t{i}.domain)\")\n",
    "#                     for j in range(i):\n",
    "#                         join_clauses.append(f\"(t{j}.object_id != t{i}.object_id)\")\n",
    "\n",
    "#                     join_statements.append(f\"JOIN t{i} ON {' AND '.join(join_clauses)}\")\n",
    "\n",
    "#             query = f\"\"\"WITH {', '.join(ctes)}\n",
    "# SELECT t0.domain, {', '.join(object_id_selects)} FROM t0\n",
    "# {' '.join(join_statements)}\n",
    "# \"\"\"\n",
    "\n",
    "#         return query\n",
    "\n",
    "#     def _build_potential_missing_values_query(self, mapping: typing.Dict[str, typing.Union[str, typing.List[str]]], relevant_vars: typing.List[str]):\n",
    "#         # For each trace ID, and each assignment of the vars that exist in the sub_predicate_df so far:\n",
    "#         relevant_var_mapping = {var: mapping[var] if var.startswith(\"?\") else [var] for var in relevant_vars}\n",
    "        \n",
    "#         object_assignments_query = self.object_assignments_query(relevant_var_mapping)\n",
    "\n",
    "#         select_variables = ', '.join(f'object_assignments.\"{var}\" as \"{var}\"' for var in relevant_vars)\n",
    "#         query = f\"\"\"SELECT trace_length_and_domains.trace_id as trace_id, trace_length_and_domains.domain as domain, {select_variables}, empty_bitstrings.intervals as intervals\n",
    "#         FROM trace_length_and_domains\n",
    "#         JOIN ({object_assignments_query}) AS object_assignments ON (trace_length_and_domains.domain = object_assignments.domain))\n",
    "#         JOIN empty_bitstrings ON (trace_length_and_domains.trace_id = empty_bitstrings.trace_id)\n",
    "#         \"\"\"\n",
    "#         if DEBUG: print(query)\n",
    "\n",
    "#         return query\n",
    "\n",
    "#     @cachetools.cachedmethod(operator.attrgetter('cache'), key=_predicate_and_mapping_cache_key)\n",
    "#     def _handle_not(self, predicate: tatsu.ast.AST, mapping: typing.Dict[str, typing.Union[str, typing.List[str]]], **kwargs) -> typing.Tuple[str, typing.Set[str]]:\n",
    "#         try:\n",
    "#             inner_query, used_variables = self._inner_filter(predicate[\"not_args\"], mapping)  # type: ignore\n",
    "#         except PredicateNotImplementedException as e:\n",
    "#             raise PredicateNotImplementedException(f\"Sub-predicate of the not ({e.args}) was not implemented\")\n",
    "\n",
    "\n",
    "#         relevant_vars = list(used_variables)\n",
    "#         potential_missing_values_query = self._build_potential_missing_values_query(mapping, relevant_vars)\n",
    "#         potential_missing_values_table_name = self._next_temp_table_name()\n",
    "#         inner_table_name = self._next_temp_table_name()\n",
    "\n",
    "#         # Now, for each possible combination of args on each trace / domain, 'intervals' will contain the truth intervals if\n",
    "#         # they exist and null otherwise, and 'intervals_right' will contain the empty interval'\n",
    "#         join_columns = [\"trace_id\"] + relevant_vars\n",
    "\n",
    "#         select_items = [f\"{potential_missing_values_table_name}.trace_id as trace_id\", f\"{potential_missing_values_table_name}.domain as domain\"]\n",
    "#         select_items.extend(f'{potential_missing_values_table_name}.\"{var}\" as \"{var}\"' for var in relevant_vars)\n",
    "#         select_items.append(f\"(~( {potential_missing_values_table_name}.intervals | COALESCE({inner_table_name}.intervals, {potential_missing_values_table_name}.intervals) )) AS intervals\")\n",
    "\n",
    "#         join_items = [f'{potential_missing_values_table_name}.\"{column}\"={inner_table_name}.\"{column}\"'  for column in join_columns]\n",
    "\n",
    "#         not_query = f\"\"\"WITH {potential_missing_values_table_name} AS ({potential_missing_values_query}), {inner_table_name} AS ({inner_query})\n",
    "#         SELECT {', '.join(select_items)} FROM {potential_missing_values_table_name} LEFT JOIN {inner_table_name} ON {' AND '.join(join_items)};\n",
    "#         \"\"\"\n",
    "\n",
    "#         table_name = self._next_temp_table_name()\n",
    "#         query = f\"WITH {table_name} AS ({not_query}) SELECT * FROM {table_name} WHERE bit_count(intervals) != 0;\"\n",
    "#         if DEBUG: print(query)\n",
    "#         return query, used_variables\n",
    "\n",
    "\n",
    "#     def _inner_filter(self, predicate: tatsu.ast.AST, mapping: typing.Dict[str, typing.Union[str, typing.List[str]]], **kwargs) -> typing.Tuple[str, typing.Set[str]]:\n",
    "#         '''\n",
    "#         Filters the data by the given predicate and mapping, returning a list of intervals in which the predicate is true\n",
    "#         for each processed trace\n",
    "\n",
    "#         Returns a dictionary mapping from the trace ID to a dictionary that maps from the set of arguments to a list of\n",
    "#         intervals in which the predicate is true for that set of arguments\n",
    "#         '''\n",
    "\n",
    "#         predicate_rule = predicate.parseinfo.rule  # type: ignore\n",
    "\n",
    "#         if predicate_rule == \"predicate\":\n",
    "#             return self._handle_predicate(predicate, mapping, **kwargs)\n",
    "\n",
    "#         elif predicate_rule == \"super_predicate\":\n",
    "#             return self._inner_filter(predicate[\"pred\"], mapping, **kwargs)  # type: ignore\n",
    "\n",
    "#         elif predicate_rule == \"super_predicate_and\":\n",
    "#             return self._handle_and(predicate, mapping, **kwargs)\n",
    "\n",
    "#         elif predicate_rule == \"super_predicate_or\":\n",
    "#             return self._handle_or(predicate, mapping, **kwargs)\n",
    "\n",
    "#         elif predicate_rule == \"super_predicate_not\":\n",
    "#             return self._handle_not(predicate, mapping, **kwargs)\n",
    "\n",
    "#         elif predicate_rule in [\"super_predicate_exists\", \"super_predicate_forall\", \"function_comparison\"]:\n",
    "#             raise PredicateNotImplementedException(predicate_rule)\n",
    "\n",
    "#         else:\n",
    "#             raise ValueError(f\"Error: Unknown rule '{predicate_rule}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 12:55:28 - compile_predicate_statistics_full_database - INFO     - Creating DuckDB table...\n",
      "2023-08-17 12:55:30 - compile_predicate_statistics_full_database - INFO     - Loaded data, found 1707407 rows\n"
     ]
    }
   ],
   "source": [
    "import compile_predicate_statistics_full_database\n",
    "\n",
    "stats = compile_predicate_statistics_full_database.CommonSensePredicateStatisticsFullDatabase(force_trace_names_hash='028b3733')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"set temp_directory='/tmp/duckdb'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('SELECT * from data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"INSERT INTO data (predicate, trace_id, domain, intervals) SELECT 'game_start' as predicate, trace_id, domain, bitstring('1', length) as intervals FROM trace_length_and_domains\")\n",
    "duckdb.sql(\"INSERT INTO data (predicate, trace_id, domain, intervals) SELECT 'game_end' as predicate, trace_id, domain, set_bit(bitstring('0', length), 0, 1) as intervals FROM trace_length_and_domains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"INSERT INTO data (predicate, trace_id, domain, intervals) SELECT 'game_end' as predicate, trace_id, domain, set_bit(bitstring('0', length), 0, 1) as intervals FROM trace_length_and_domains\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
