{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 11:24:54 - ast_utils - DEBUG    - Using cache folder: /Users/guydavidson/tmp/game_generation_cache\n",
      "2023-10-02 11:24:54 - src.ast_utils - DEBUG    - Using cache folder: /Users/guydavidson/tmp/game_generation_cache\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import gzip\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tatsu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import tqdm.notebook as tqdmn\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from src import fitness_energy_utils as utils\n",
    "from src.fitness_energy_utils import NON_FEATURE_COLUMNS\n",
    "from src.ast_counter_sampler import *\n",
    "from src.ast_utils import cached_load_and_parse_games_from_file, load_games_from_file, _extract_game_id\n",
    "from src import ast_printer\n",
    "from src.fitness_features_preprocessing import NGRAM_SCORE_PATTERN\n",
    "from src.fitness_features_by_category import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 11:24:56 - src.ast_utils - INFO     - Loading from cache file: /Users/guydavidson/tmp/game_generation_cache/interactive-beta-cache.pkl.gz\n",
      "2023-10-02 11:24:56 - src.ast_utils - INFO     - Finished loading cache file: /Users/guydavidson/tmp/game_generation_cache/interactive-beta-cache.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='..'))\n",
    "# real_game_texts = [ast_printer.ast_to_string(ast, '\\n') for ast in game_asts]\n",
    "# regrown_game_texts = list(load_games_from_file('../dsl/ast-real-regrowth-samples.pddl'))\n",
    "\n",
    "# regrown_game_asts = list(cached_load_and_parse_games_from_file('../dsl/ast-real-regrowth-samples-1024.pddl', grammar_parser, True, relative_path='..'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interactive-beta.pddl' 'ast-real-regrowth-samples-1024.pddl.gz']\n",
      "(100450, 165)\n",
      "All original games have 1024 regrowths\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>src_file</th>\n",
       "      <th>game_name</th>\n",
       "      <th>domain_name</th>\n",
       "      <th>original_game_name</th>\n",
       "      <th>real</th>\n",
       "      <th>variables_defined_all</th>\n",
       "      <th>variables_defined_prop</th>\n",
       "      <th>variables_used_all</th>\n",
       "      <th>variables_used_prop</th>\n",
       "      <th>...</th>\n",
       "      <th>ast_ngram_constraints_n_4_score</th>\n",
       "      <th>ast_ngram_constraints_n_5_score</th>\n",
       "      <th>ast_ngram_terminal_n_2_score</th>\n",
       "      <th>ast_ngram_terminal_n_3_score</th>\n",
       "      <th>ast_ngram_terminal_n_4_score</th>\n",
       "      <th>ast_ngram_terminal_n_5_score</th>\n",
       "      <th>ast_ngram_scoring_n_2_score</th>\n",
       "      <th>ast_ngram_scoring_n_3_score</th>\n",
       "      <th>ast_ngram_scoring_n_4_score</th>\n",
       "      <th>ast_ngram_scoring_n_5_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>interactive-beta.pddl</td>\n",
       "      <td>6172feb1665491d1efbce164-0</td>\n",
       "      <td>medium-objects-room-v1</td>\n",
       "      <td>6172feb1665491d1efbce164-0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968048</td>\n",
       "      <td>0.979128</td>\n",
       "      <td>0.968876</td>\n",
       "      <td>0.979584</td>\n",
       "      <td>0.980420</td>\n",
       "      <td>0.982740</td>\n",
       "      <td>0.919068</td>\n",
       "      <td>0.960887</td>\n",
       "      <td>0.998712</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>interactive-beta.pddl</td>\n",
       "      <td>5f77754ba932fb2c4ba181d8-2</td>\n",
       "      <td>many-objects-room-v1</td>\n",
       "      <td>5f77754ba932fb2c4ba181d8-2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973823</td>\n",
       "      <td>0.978192</td>\n",
       "      <td>0.965107</td>\n",
       "      <td>0.976486</td>\n",
       "      <td>0.975603</td>\n",
       "      <td>0.983432</td>\n",
       "      <td>0.899002</td>\n",
       "      <td>0.914971</td>\n",
       "      <td>0.965387</td>\n",
       "      <td>0.974323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>interactive-beta.pddl</td>\n",
       "      <td>614b603d4da88384282967a7-3</td>\n",
       "      <td>many-objects-room-v1</td>\n",
       "      <td>614b603d4da88384282967a7-3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945618</td>\n",
       "      <td>0.972216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850539</td>\n",
       "      <td>0.904462</td>\n",
       "      <td>0.948491</td>\n",
       "      <td>0.957243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>interactive-beta.pddl</td>\n",
       "      <td>5bc79f652885710001a0e82a-5</td>\n",
       "      <td>few-objects-room-v1</td>\n",
       "      <td>5bc79f652885710001a0e82a-5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986638</td>\n",
       "      <td>0.980074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.919068</td>\n",
       "      <td>0.960887</td>\n",
       "      <td>0.998712</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>interactive-beta.pddl</td>\n",
       "      <td>614dec67f6eb129c3a77defd-6</td>\n",
       "      <td>medium-objects-room-v1</td>\n",
       "      <td>614dec67f6eb129c3a77defd-6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.982992</td>\n",
       "      <td>0.979321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.964519</td>\n",
       "      <td>0.971914</td>\n",
       "      <td>0.973287</td>\n",
       "      <td>0.964638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index               src_file                   game_name  \\\n",
       "0      0  interactive-beta.pddl  6172feb1665491d1efbce164-0   \n",
       "1      1  interactive-beta.pddl  5f77754ba932fb2c4ba181d8-2   \n",
       "2      2  interactive-beta.pddl  614b603d4da88384282967a7-3   \n",
       "3      3  interactive-beta.pddl  5bc79f652885710001a0e82a-5   \n",
       "4      4  interactive-beta.pddl  614dec67f6eb129c3a77defd-6   \n",
       "\n",
       "              domain_name          original_game_name  real  \\\n",
       "0  medium-objects-room-v1  6172feb1665491d1efbce164-0     1   \n",
       "1    many-objects-room-v1  5f77754ba932fb2c4ba181d8-2     1   \n",
       "2    many-objects-room-v1  614b603d4da88384282967a7-3     1   \n",
       "3     few-objects-room-v1  5bc79f652885710001a0e82a-5     1   \n",
       "4  medium-objects-room-v1  614dec67f6eb129c3a77defd-6     1   \n",
       "\n",
       "   variables_defined_all  variables_defined_prop  variables_used_all  \\\n",
       "0                      1                     1.0                   1   \n",
       "1                      1                     1.0                   1   \n",
       "2                      1                     1.0                   1   \n",
       "3                      1                     1.0                   1   \n",
       "4                      1                     1.0                   1   \n",
       "\n",
       "   variables_used_prop  ...  ast_ngram_constraints_n_4_score  \\\n",
       "0                  1.0  ...                         0.968048   \n",
       "1                  1.0  ...                         0.973823   \n",
       "2                  1.0  ...                         0.945618   \n",
       "3                  1.0  ...                         0.986638   \n",
       "4                  1.0  ...                         0.982992   \n",
       "\n",
       "   ast_ngram_constraints_n_5_score  ast_ngram_terminal_n_2_score  \\\n",
       "0                         0.979128                      0.968876   \n",
       "1                         0.978192                      0.965107   \n",
       "2                         0.972216                      0.000000   \n",
       "3                         0.980074                      0.000000   \n",
       "4                         0.979321                      0.000000   \n",
       "\n",
       "   ast_ngram_terminal_n_3_score  ast_ngram_terminal_n_4_score  \\\n",
       "0                      0.979584                      0.980420   \n",
       "1                      0.976486                      0.975603   \n",
       "2                      0.000000                      0.000000   \n",
       "3                      0.000000                      0.000000   \n",
       "4                      0.000000                      0.000000   \n",
       "\n",
       "   ast_ngram_terminal_n_5_score  ast_ngram_scoring_n_2_score  \\\n",
       "0                      0.982740                     0.919068   \n",
       "1                      0.983432                     0.899002   \n",
       "2                      0.000000                     0.850539   \n",
       "3                      0.000000                     0.919068   \n",
       "4                      0.000000                     0.964519   \n",
       "\n",
       "   ast_ngram_scoring_n_3_score  ast_ngram_scoring_n_4_score  \\\n",
       "0                     0.960887                     0.998712   \n",
       "1                     0.914971                     0.965387   \n",
       "2                     0.904462                     0.948491   \n",
       "3                     0.960887                     0.998712   \n",
       "4                     0.971914                     0.973287   \n",
       "\n",
       "   ast_ngram_scoring_n_5_score  \n",
       "0                     1.000000  \n",
       "1                     0.974323  \n",
       "2                     0.957243  \n",
       "3                     1.000000  \n",
       "4                     0.964638  \n",
       "\n",
       "[5 rows x 165 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness_df = utils.load_fitness_data('../data/fitness_features_1024_regrowths.csv.gz')\n",
    "print(fitness_df.src_file.unique())\n",
    "print(fitness_df.shape)\n",
    "original_game_counts = fitness_df.groupby('original_game_name').src_file.count().value_counts()\n",
    "if len(original_game_counts) == 1:\n",
    "    print(f'All original games have {original_game_counts.index[0] - 1} regrowths')\n",
    "else:\n",
    "    print('Some original games have different numbers of regrowths: {original_game_counts}')\n",
    "fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_ = fitness_df[fitness_df.real == 1].mean() \n",
    "one_mean_features = m_[m == 1]\n",
    "print(list(one_mean_features.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_mean_features = list(m_[m == 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_categories = [\"forall_less_important\", \"counting_less_important\", \"grammar_use_less_important\", \"predicate_under_modal\", \"predicate_role_filler\", \"compositionality\"]\n",
    "feature_columns = [c for c in fitness_df.columns if c not in NON_FEATURE_COLUMNS]\n",
    "all_ignore_features = set()\n",
    "\n",
    "for category in ignore_categories:\n",
    "    for feature in FEATURE_CATEGORIES[category]:\n",
    "        if isinstance(feature, re.Pattern):\n",
    "            all_ignore_features.update([f for f in feature_columns if feature.match(f)])\n",
    "        else:\n",
    "            all_ignore_features.add(feature)\n",
    "\n",
    "filtered_zero_mean_features = [c for c in zero_mean_features if c not in all_ignore_features]\n",
    "\n",
    "print(filtered_zero_mean_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = fitness_df.groupby('real')[[c for c in fitness_df.columns if c not in ('Index', 'real')]].mean()\n",
    "mean_diffs = g.loc[1] - g.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diffs[[c for c in fitness_df.columns if c in ('adjacent_once_found', 'no_adjacent_same_modal', 'starts_and_ends_once', 'once_in_middle_of_pref_found', 'pref_without_hold_found')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asm = fitness_df.groupby('real').no_adjacent_same_modal.value_counts() / fitness_df.groupby('real').no_adjacent_same_modal.count()\n",
    "asm[1] - asm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "logger.debug('Features with largest negative diffs:\\n' + str(mean_diffs.nsmallest(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVES_FILE = 'interactive-beta.pddl'\n",
    "NEGATIVES_FILE = 'ast-real-regrowth-samples-1024.pddl'\n",
    "\n",
    "def create_filtered_df(df: pd.DataFrame, \n",
    "    filter_data_src_files: typing.Sequence[str] = (POSITIVES_FILE, NEGATIVES_FILE),\n",
    "    ) -> pd.DataFrame:\n",
    "    f_df = fitness_df[fitness_df.src_file.isin(filter_data_src_files)].reset_index(drop=True)\n",
    "    f_df.loc[f_df.src_file == filter_data_src_files[0], 'real'] = 1\n",
    "    return f_df\n",
    "\n",
    "filtered_fitness_df = create_filtered_df(fitness_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_fitness_df.shape)\n",
    "filtered_fitness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_fitness_df[(filtered_fitness_df.real == 1) & (filtered_fitness_df.two_number_operation_found == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latest_model_paths import LATEST_FITNESS_FEATURIZER_PATH, LATEST_FITNESS_FUNCTION_DATE_ID\n",
    "from src.fitness_features import *\n",
    "\n",
    "def _load_pickle_gzip(path: str):\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "fitness_featurizer = _load_pickle_gzip(LATEST_FITNESS_FEATURIZER_PATH)\n",
    "fitnes_function, feature_names = utils.load_model_and_feature_columns(LATEST_FITNESS_FUNCTION_DATE_ID)\n",
    "real_game_feafure_dicts = [fitness_featurizer.parse(ast, return_row=True) for ast in game_asts]\n",
    "real_game_feature_lists = [[fd[name] for name in feature_names] for fd in real_game_feafure_dicts]\n",
    "real_game_feature_vectors = [np.array(fl, dtype=float) for fl in real_game_feature_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_N_DIM = 32\n",
    "SEED = 100\n",
    "PCA_KWARGS = dict(random_state=SEED)\n",
    "TSNE_KAWRGS = dict(init='pca', learning_rate='auto', random_state=SEED)\n",
    "\n",
    "def pca_and_tsne(data: np.ndarray, pca_n_dim: int = PCA_N_DIM, \n",
    "    pca_kwargs: typing.Optional[typing.Dict] = None, tsne_kwargs: typing.Optional[typing.Dict] = None):\n",
    "\n",
    "    if pca_kwargs is None:\n",
    "        pca_kwargs = PCA_KWARGS\n",
    "    else:\n",
    "        temp_kwargs = PCA_KWARGS.copy()\n",
    "        temp_kwargs.update(pca_kwargs)\n",
    "        pca_kwargs = temp_kwargs\n",
    "\n",
    "    if tsne_kwargs is None:\n",
    "        tsne_kwargs = TSNE_KAWRGS\n",
    "\n",
    "    else:\n",
    "        temp_kwrags = TSNE_KAWRGS.copy()\n",
    "        temp_kwrags.update(tsne_kwargs)\n",
    "        tsne_kwargs = temp_kwrags\n",
    "\n",
    "    if data.ndim > 2:\n",
    "        data = data.reshape(-1, data.shape[-1])\n",
    "\n",
    "    pca = PCA(n_components=pca_n_dim, **pca_kwargs)\n",
    "    data_pca = pca.fit_transform(data)\n",
    "    tsne = TSNE(n_components=1, **tsne_kwargs)\n",
    "    data_tsne = tsne.fit_transform(data_pca)\n",
    "\n",
    "    return data_tsne\n",
    "\n",
    "\n",
    "tsne_results = pca_and_tsne(np.array(real_game_feature_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_throwing = np.array([ True,  True, False,  True,  True,  True,  True,  True, False,\n",
    "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
    "        True,  True,  True, False,  True, False,  True,  True, False,\n",
    "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
    "        True,  True,  True,  True,  True, False,  True,  True, False,\n",
    "       False,  True,  True, False, False,  True,  True,  True, False,\n",
    "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True, False,  True,  True,\n",
    "        True,  True,  True, False,  True,  True,  True, False])\n",
    "\n",
    "\n",
    "game_types = np.array(['throwing', 'throwing', 'building', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'building', 'throwing',\n",
    "       'throwing', 'throwing', 'building_throwing', 'throwing',\n",
    "       'building', 'throwing', 'building_throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'organizing', 'throwing',\n",
    "       'organizing', 'throwing', 'building_throwing', 'organizing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'organizing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'building_organizing_throwing', 'throwing',\n",
    "       'organizing', 'throwing', 'throwing', 'organizing', 'building',\n",
    "       'throwing', 'throwing', 'organizing', 'building', 'throwing',\n",
    "       'building_throwing', 'throwing', 'building', 'throwing',\n",
    "       'organizing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'organizing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', '', 'throwing', 'throwing', 'throwing',\n",
    "       'throwing', 'throwing', 'building', 'throwing', 'throwing',\n",
    "       'throwing', 'organizing'], dtype=object)\n",
    "\n",
    "unique_game_types = set(game_types)\n",
    "game_types_code = {t: i for i, t in enumerate(unique_game_types)}\n",
    "game_types_list = [game_types_code[t] for t in game_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for game_type in unique_game_types:\n",
    "    indices = np.where(game_types == game_type)[0]\n",
    "    if not game_type:\n",
    "        game_type = 'uncategorized'\n",
    "    if game_type == 'building_organizing_throwing':\n",
    "        game_type = 'all'\n",
    "    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 0], label=game_type)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx, max_idx = tsne_results.argmin(), tsne_results.argmax()\n",
    "print(min_idx, max_idx)\n",
    "print()\n",
    "print(ast_printer.ast_to_string(game_asts[min_idx], '\\n'))\n",
    "print()\n",
    "print(ast_printer.ast_to_string(game_asts[max_idx], '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_games = filtered_fitness_df[filtered_fitness_df.src_file == NEGATIVES_FILE].reset_index()\n",
    "broadcasted_original = filtered_fitness_df.loc[[filtered_fitness_df.index[(filtered_fitness_df.game_name == original_name)][0] for original_name in fake_games.original_game_name], :].reset_index()\n",
    "\n",
    "original_regrown_diffs = (broadcasted_original.drop(NON_FEATURE_COLUMNS, axis=1) - fake_games.drop(NON_FEATURE_COLUMNS, axis=1))\n",
    "\n",
    "unchanged_games_prop = (original_regrown_diffs.drop('index', axis=1) == 0).all(axis=1).sum() / len(original_regrown_diffs)\n",
    "print(f'In {unchanged_games_prop * 100:.2f}% of the games, the regrown game was identical to the original game.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [str(c) for c in fitness_df.columns if c not in NON_FEATURE_COLUMNS]\n",
    "\n",
    "remove_all_ngram_scores = []  #  ('full',)\n",
    "for score_type in ('full', 'setup', 'constraints', 'terminal', 'scoring'):\n",
    "    col_names = [c for c in feature_columns if c.startswith(f'ast_ngram_{score_type}') and c.endswith('_score')]\n",
    "\n",
    "    if score_type not in remove_all_ngram_scores:\n",
    "        col_names = col_names[:-1]\n",
    "\n",
    "    for col in col_names:\n",
    "        feature_columns.remove(col)\n",
    "\n",
    "other_features = ['all_variables_defined', 'all_variables_used',\n",
    "    'starts_and_ends_once',  # 'setup_objects_used',\n",
    "    'all_preferences_used', 'no_adjacent_same_modal', 'adjacent_once_found',\n",
    "    'repeated_variables_found', 'nested_logicals_found', 'identical_logical_children_found', \n",
    "    'no_two_number_operations', 'tautological_expression_found', 'redundant_expression_found',]\n",
    "\n",
    "# Next up: compositionality_structure_, max_depth, mean_depth_, node_count_, predicate_under_modal_, max_number_variables_types_quantified_, max_quantification_count_, _arg_types_, length_of_then_modals_\n",
    "prefixes = ['section_', 'pref_forall_', 'compositionality_structure_', 'max_depth_', 'mean_depth_']\n",
    "\n",
    "feature_columns = [c for c in feature_columns if 'score' in c or any(c.startswith(prefix) for prefix in prefixes) or c in other_features]\n",
    "feature_columns_set = set(feature_columns)\n",
    "\n",
    "fake_games = filtered_fitness_df[filtered_fitness_df.src_file == NEGATIVES_FILE].reset_index()\n",
    "broadcasted_original = filtered_fitness_df.loc[[filtered_fitness_df.index[(filtered_fitness_df.game_name == original_name)][0] for original_name in fake_games.original_game_name], :].reset_index()\n",
    "\n",
    "original_regrown_diffs = (broadcasted_original.drop([c for c in broadcasted_original.columns if c not in feature_columns_set], axis=1) - fake_games.drop([c for c in fake_games.columns if c not in feature_columns_set], axis=1))\n",
    "\n",
    "if 'index' in original_regrown_diffs.columns:\n",
    "    original_regrown_diffs = original_regrown_diffs.drop('index', axis=1)\n",
    "\n",
    "unchanged_games_prop = (original_regrown_diffs == 0).all(axis=1).sum() / len(original_regrown_diffs)\n",
    "print(f'In {unchanged_games_prop * 100:.2f}% of the games, the regrown game was identical to the original game.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_change = (original_regrown_diffs.drop('index', axis=1) == 0).all(axis=0)\n",
    "for x in never_change.index[never_change]:\n",
    "    print(x)\n",
    "\n",
    "print(sum(never_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_features_by_real = filtered_fitness_df[['real'] + [c for c in filtered_fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic model-fitting experiment approach\n",
    "We have a large dataset now, I can try to cross-validate over some of the choices I might make:\n",
    "* Change the random seed?\n",
    "* See if the GPU is faster\n",
    "* Try a different from of regularization?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_by_abs_diff_threshold(diffs: pd.Series, score_threshold: float):\n",
    "    feature_columns = list(diffs[diffs >= score_threshold].index)\n",
    "\n",
    "    remove_all_ngram_scores = []  \n",
    "    for score_type in ('full', 'setup', 'constraints', 'terminal', 'scoring'):\n",
    "        col_names = sorted([c for c in feature_columns if c.startswith(f'ast_ngram_{score_type}') and c.endswith('_score')])\n",
    "\n",
    "        if score_type not in remove_all_ngram_scores:\n",
    "            col_names = col_names[:-1]\n",
    "\n",
    "        for col in col_names:\n",
    "            feature_columns.remove(col)\n",
    "\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = 1.0\n",
    "\n",
    "test_param_grid = [\n",
    "    {\n",
    "        'fitness__loss_function': [utils.fitness_softmin_loss, utils.fitness_softmin_loss_positive_to_all_negatives], # [utils.fitness_hinge_loss_with_cross_example],\n",
    "        # 'fitness__weight_decay': [0.0],  \n",
    "        # 'fitness__margin': [8, 16],\n",
    "        'fitness__beta': [BETA], #   [4, 8],\n",
    "        'fitness__lr': [4e-3, 1e-3, 3e-4],  #  [1e-2, 3e-3],  # [1e-1, 3e-2, 1e-2, 3e-3],\n",
    "        'fitness__k': [256, 512, 1024],  # [256, 512, 1024],  # 128\n",
    "        'fitness__batch_size': [1, 2, 4, 8, 16]  # , 16],  # [1, 4, 8, 16],\n",
    "        # 'fitness__alpha': [0, 0.25, 0.5, 0.75, 1], # [0, 0.1, 0.2, 0.3],  #\n",
    "        # 'fitness__dataset_energy_beta': [1, 3, 5],\n",
    "        # 'fitness__regularization_weight': [0.01, 0.05],\n",
    "    },\n",
    "]\n",
    "\n",
    "def build_regularization_function(ord: int = 1, threshold: float = 0):\n",
    "    def regularization_function(model: nn.Module) -> torch.Tensor:\n",
    "        w = model.fc1.weight.squeeze()  # type: ignore\n",
    "        return torch.linalg.norm(w * (w.abs() >= threshold), ord)\n",
    "    \n",
    "    return regularization_function\n",
    "\n",
    "regularizer = build_regularization_function(ord=1, threshold=0)\n",
    "\n",
    "scaler_kwargs = dict(passthrough=True)\n",
    "model_kwargs = dict(output_activation=nn.Identity())\n",
    "train_kwargs = dict(\n",
    "    negative_score_reduction='none', \n",
    "    n_epochs=20000, patience_epochs=200, \n",
    "    bias_init_margin_ratio=0.01,\n",
    "    device=torch.device('cuda:0'), \n",
    "    # regularizer=regularizer,\n",
    "    shuffle_negatives=True, \n",
    "    split_validation_from_train=True,\n",
    "    evaluate_opposite_shuffle_mode=False,\n",
    "    full_dataset_on_device=True,\n",
    "    # use_lr_scheduler=True,\n",
    "    )\n",
    "cv_kwargs = dict(refit='loss', error_score='raise')  # , n_jobs=6)  # , n_jobs=1)\n",
    "scoring = utils.build_multiple_scoring_function(\n",
    "    [utils.wrap_loss_function_to_metric(utils.fitness_sofmin_loss_positive_negative_split, dict(beta=BETA), True),  # type: ignore\n",
    "     utils.evaluate_fitness_overall_ecdf, utils.evaluate_fitness_single_game_rank, utils.evaluate_fitness_single_game_min_rank, \n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.01), True),  # type: ignore\n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.05), True),  # type: ignore\n",
    "     ],\n",
    "    ['loss', 'overall_ecdf', 'single_game_rank', 'single_game_min_rank', 'energy_of_negative@1%', 'energy_of_negative@5%'],\n",
    ")\n",
    "\n",
    "score_threshold = 0.02\n",
    "\n",
    "mean_features_by_real = filtered_fitness_df[['real'] + [c for c in filtered_fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()\n",
    "feature_columns = get_features_by_abs_diff_threshold(abs_diffs, score_threshold)\n",
    "\n",
    "# remove_all_ngram_scores = []  #  ('full',)\n",
    "# for score_type in ('full', 'setup', 'constraints', 'terminal', 'scoring'):\n",
    "#     col_names = [c for c in feature_columns if c.startswith(f'ast_ngram_{score_type}') and c.endswith('_score')]\n",
    "\n",
    "#     if score_type not in remove_all_ngram_scores:\n",
    "#         col_names = col_names[:-1]\n",
    "\n",
    "#     for col in col_names:\n",
    "#         feature_columns.remove(col)\n",
    "\n",
    "# other_features = ['all_variables_defined', 'all_variables_used',\n",
    "#     'starts_and_ends_once', 'setup_objects_used',\n",
    "#     'all_preferences_used', 'no_adjacent_same_modal', 'adjacent_once_found',\n",
    "#     'repeated_variables_found', 'nested_logicals_found', 'identical_logical_children_found', \n",
    "#     'no_two_number_operations', 'tautological_expression_found', 'redundant_expression_found',]\n",
    "\n",
    "\n",
    "# Next up: compositionality_structure_, max_depth, mean_depth_, node_count_, predicate_under_modal_, max_number_variables_types_quantified_, max_quantification_count_, _arg_types_, length_of_then_modals_\n",
    "# prefixes = ['section_', 'pref_forall_', 'compositionality_structure_', 'max_depth_', 'mean_depth_']  #  \n",
    "# middles = ['score']\n",
    "\n",
    "# feature_columns = [c for c in feature_columns if any(middle in c for middle in middles) or any(c.startswith(prefix) for prefix in prefixes) or c in other_features]\n",
    "# feature_columns = [c for c in feature_columns if 'score' in c or c.startswith('section_')]\n",
    "\n",
    "cv, (train_tensor, test_tensor), results = utils.model_fitting_experiment(\n",
    "    # [fitness_df, mle_samples_df], \n",
    "    fitness_df, \n",
    "    test_param_grid, feature_columns=feature_columns,\n",
    "    scoring_function=scoring, verbose=1, scaler_kwargs=scaler_kwargs, \n",
    "    model_kwargs=model_kwargs, train_kwargs=train_kwargs, cv_kwargs=cv_kwargs,\n",
    "    # energy_weighted_resampling=True, \n",
    "    # random_seed=121,\n",
    "    )\n",
    "\n",
    "utils.visualize_cv_outputs(cv, train_tensor, test_tensor, results, title_note='feature search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('../data/fitness_cv/fitness_sweep_fixed_features_2023_03_22_2.pkl.gz', 'rb') as f:\n",
    "    fitness_sweep = pickle.load(f)\n",
    "\n",
    "\n",
    "cv = fitness_sweep['cv']\n",
    "train_tensor = fitness_sweep['train_tensor']\n",
    "test_tensor = fitness_sweep['test_tensor']\n",
    "results = fitness_sweep['results']\n",
    "feature_columns = fitness_sweep['feature_columns']\n",
    "\n",
    "utils.visualize_cv_outputs(cv, train_tensor, test_tensor, results, title_note='latest sweep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_ON_FULL_DATA = True\n",
    "SAVE_MODEL = True\n",
    "\n",
    "\n",
    "if FIT_ON_FULL_DATA:\n",
    "    full_tensor = utils.df_to_tensor(fitness_df, feature_columns)\n",
    "    cv.best_estimator_['fitness'].train_kwargs['split_validation_from_train'] = False\n",
    "    cv.best_estimator_.fit(full_tensor)\n",
    "    print(utils.evaluate_trained_model(cv.best_estimator_, full_tensor))\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    utils.save_model_and_feature_columns(cv, feature_columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cv.best_estimator_.named_steps['fitness'].model.fc1.weight.squeeze().detach().cpu()\n",
    "\n",
    "K = 15\n",
    "top_features = torch.topk(weights, K)\n",
    "bottom_features = torch.topk(weights, K, largest=False)\n",
    "\n",
    "lines = []\n",
    "\n",
    "lines.append('**Features with largest negative weights (most real):**')\n",
    "for i in range(K):\n",
    "    lines.append(f'{i+1}. {feature_columns[bottom_features.indices[i]]} ({bottom_features.values[i]:.4f})')\n",
    "\n",
    "lines.append('\\n**Features with largest positive weights (most fake):**')\n",
    "for i in range(K):\n",
    "    lines.append((f'{i+1}. {feature_columns[top_features.indices[i]]} ({top_features.values[i]:.4f})'))\n",
    "\n",
    "display(Markdown('\\n'.join(lines)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cv.best_estimator_.named_steps['fitness'].model.fc1.weight.squeeze().detach().cpu()\n",
    "weight_indices = torch.argsort(weights)\n",
    "found_positive = False\n",
    "\n",
    "lines = ['**Feature Weights (ascending):**\\n']\n",
    "for i, idx in enumerate(weight_indices):\n",
    "    lines.append(f'{i:>2}.  {feature_columns[idx]} = {weights[idx].item():.3f}')\n",
    "    if not found_positive and weights[weight_indices[i + 1]] > 0:\n",
    "        found_positive = True\n",
    "        lines.append('----')\n",
    "\n",
    "display(Markdown('\\n'.join(lines)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_fitness_df[(filtered_fitness_df.real == 1) & (filtered_fitness_df.section_without_pref_or_total_count_terminal > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = filtered_fitness_df.groupby('real')[[c for c in filtered_fitness_df.columns if 'max_quantification_count_' in c]].mean()\n",
    "np.abs(gb.loc[1] - gb.loc[0]).mean() * (98 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_fitness_df.groupby('real').section_without_pref_or_total_count_scoring.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_fitness_df.groupby('real').section_without_pref_or_total_count_terminal.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(c, weights[feature_columns.index(c)].item()) for c in feature_columns if 'exists' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT_ON_FULL_DATA = True\n",
    "SAVE_MODEL = True\n",
    "\n",
    "\n",
    "if FIT_ON_FULL_DATA:\n",
    "    full_tensor = utils.df_to_tensor(fitness_df, feature_columns)\n",
    "    cv.best_estimator_['fitness'].train_kwargs['split_validation_from_train'] = False\n",
    "    cv.best_estimator_.fit(full_tensor)\n",
    "    print(utils.evaluate_trained_model(cv.best_estimator_, full_tensor))\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    utils.save_model_and_feature_columns(cv, feature_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic evaluation without cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_kwargs = dict(passthrough=True)\n",
    "model_kwargs = dict()\n",
    "train_kwargs = dict(\n",
    "    loss_function=utils.fitness_softmin_loss,\n",
    "    k=1024,\n",
    "    lr=1e-2,\n",
    "    beta=4.0, \n",
    "    negative_score_reduction='none', \n",
    "    n_epochs=3000, \n",
    "    shuffle_negatives=True, \n",
    "    bias_init_margin_ratio=0.01,\n",
    "    # device=torch.device('cuda:0'), \n",
    "    # regularizer=regularizer,\n",
    "    split_validation_from_train=True,\n",
    "    )\n",
    "\n",
    "sweep_param_grid = dict(\n",
    "    patience_epochs=range(10, 60, 10),\n",
    "    use_lr_scheduler=[False, True],\n",
    "    batch_size=[1, 2, 4, 8, 16],\n",
    "    score_threshold=[0, 0.005, 0.01, 0.02, 0.03, 0.04],\n",
    ")\n",
    "\n",
    "scoring = utils.build_multiple_scoring_function(\n",
    "    [utils.wrap_loss_function_to_metric(utils.fitness_sofmin_loss_positive_negative_split, dict(beta=BETA), True),\n",
    "     utils.evaluate_fitness_overall_ecdf, utils.evaluate_fitness_single_game_rank, utils.evaluate_fitness_single_game_min_rank, \n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.01), True),\n",
    "     utils.wrap_loss_function_to_metric(utils.energy_of_negative_at_quantile, dict(quantile=0.05), True),\n",
    "     ],\n",
    "    ['loss', 'overall_ecdf', 'single_game_rank', 'single_game_min_rank', 'energy_of_negative@1%', 'energy_of_negative@5%'],\n",
    ")\n",
    "\n",
    "mean_features_by_real = filtered_fitness_df[['real'] + [c for c in filtered_fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()\n",
    "\n",
    "sweep_models = {}\n",
    "sweep_results = {}\n",
    "\n",
    "for (patience_epochs, use_lr_scheduler, batch_size, score_threshold) in tqdm.tqdm(itertools.product(*sweep_param_grid.values())):\n",
    "    setting_train_kwargs = train_kwargs.copy()\n",
    "    setting_train_kwargs.update(dict(patience_epochs=patience_epochs, use_lr_scheduler=use_lr_scheduler, batch_size=batch_size))\n",
    "\n",
    "    feature_columns = list(abs_diffs[abs_diffs >= score_threshold].index)\n",
    "\n",
    "    remove_all_ngram_scores = []  \n",
    "    for score_type in ('full', 'setup', 'constraints', 'terminal', 'scoring'):\n",
    "        col_names = [c for c in feature_columns if c.startswith(f'ast_ngram_{score_type}') and c.endswith('_score')]\n",
    "\n",
    "        if score_type not in remove_all_ngram_scores:\n",
    "            col_names = col_names[:-1]\n",
    "\n",
    "        for col in col_names:\n",
    "            feature_columns.remove(col)\n",
    "\n",
    "    model, _, results = utils.initialize_and_fit_model(\n",
    "        fitness_df, split_test_set=True, feature_columns=feature_columns,\n",
    "        random_seed=DEFAULT_RANDOM_SEED,\n",
    "        scaler_kwargs=scaler_kwargs, model_kwargs=model_kwargs, train_kwargs=setting_train_kwargs,\n",
    "        # energy_weighted_resampling: bool = False, \n",
    "        # train_prop: float = DEFAULT_TRAINING_PROP,\n",
    "        scoring_function=scoring, \n",
    "    )\n",
    "\n",
    "    setting_key = (patience_epochs, use_lr_scheduler, batch_size, score_threshold, len(feature_columns))\n",
    "    sweep_models[setting_key] = model\n",
    "    sweep_results[setting_key] = results\n",
    "\n",
    "\n",
    "KEY_HEADERS = ['patience_epochs', 'use_lr_scheduler', 'batch_size', 'score_threshold', 'n_features']\n",
    "example_values = next(iter(sweep_results.values()))\n",
    "VALUE_HEADERS = [f'{outer_key}_{inner_key}' for outer_key in example_values for inner_key in example_values[outer_key]]\n",
    "\n",
    "rows = [list(key) + [results[outer_key][inner_key] for outer_key in results for inner_key in results[outer_key]]\n",
    "        for key, results in sweep_results.items()]\n",
    "\n",
    "sweep_results_df = pd.DataFrame(rows, columns=KEY_HEADERS + VALUE_HEADERS)\n",
    "sweep_results_df = sweep_results_df.assign(**{c: sweep_results_df[c].abs() for c in sweep_results_df.columns if 'ecdf' in c or 'loss' in c}, use_lr_scheduler=sweep_results_df.use_lr_scheduler.astype(int)\n",
    "sweep_results_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MAPPINGS = {\n",
    "    'patience_epochs': 'Patience Epochs',\n",
    "    'n_features': '# of Features Used',\n",
    "    'use_lr_scheduler': 'Use LR Scheduler',\n",
    "    'batch_size': 'Batch Size',\n",
    "    'train_ecdf': 'Train ECDF',\n",
    "    'test_ecdf': 'Test ECDF',\n",
    "    'train_game_rank': 'Train Game Rank',\n",
    "    'test_game_rank': 'Test Game Rank',\n",
    "}\n",
    "\n",
    "\n",
    "def plot_sweep_results(\n",
    "    results_df: pd.DataFrame, \n",
    "    x_key: str, \n",
    "    color_by_key: str,\n",
    "    column_by_key: typing.Optional[str] = None,\n",
    "    row_by_key: typing.Optional[str] = None,\n",
    "    filter_conditions: typing.Optional[typing.Dict[str, typing.Any]] = None,\n",
    "    legend_ax_index: int = 0,\n",
    "    name_mappings: typing.Dict[str, str] = NAME_MAPPINGS,\n",
    "    metrics: typing.List[str] = ['train_overall_ecdf', 'test_overall_ecdf'],\n",
    "    cmap_name: str = 'tab20',\n",
    "    ylabel: typing.Optional[str] = None,\n",
    "    subplot_adjust_params: typing.Optional[typing.Dict[str, float]] = None,\n",
    "    suptitle: typing.Optional[str] = None,\n",
    "    ):\n",
    "\n",
    "    color_values = list(sorted(results_df[color_by_key].unique()))\n",
    "    x_values = list(sorted(results_df[x_key].unique()))\n",
    "\n",
    "    column_values = []\n",
    "    if column_by_key is not None:\n",
    "        column_values = list(sorted(results_df[column_by_key].unique()))\n",
    "\n",
    "    row_values = []\n",
    "    if row_by_key is not None:\n",
    "        row_values = list(sorted(results_df[row_by_key].unique()))\n",
    "\n",
    "    if filter_conditions is not None:\n",
    "        row_filter = np.ones(len(results_df), dtype=bool)\n",
    "        for col, val in filter_conditions.items():\n",
    "            row_filter &= (results_df[col] == val)\n",
    "\n",
    "        df = results_df[row_filter]\n",
    "    else:\n",
    "        df = results_df\n",
    "\n",
    "\n",
    "    groupby_fields = []\n",
    "    n_rows = n_columns = 1\n",
    "\n",
    "    if row_by_key is not None:\n",
    "        groupby_fields.append(row_by_key)\n",
    "        n_rows = len(row_values)\n",
    "\n",
    "    if column_by_key is not None:\n",
    "        groupby_fields.append(column_by_key)\n",
    "        n_columns = len(column_values)\n",
    "        \n",
    "    groupby_fields.append(color_by_key)\n",
    "    groupby_fields.append(x_key)    \n",
    "    results_groupby = df.groupby(groupby_fields)[metrics].mean()\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_columns, figsize=(6 * n_columns, 4 * n_rows), squeeze=False)\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "\n",
    "    for row_index, row_axes in enumerate(axes):\n",
    "        row_value = None if row_by_key is None else row_values[row_index]\n",
    "        for col_index, ax in enumerate(row_axes):\n",
    "            col_value = None if column_by_key is None else column_values[col_index]\n",
    "            \n",
    "            for color_index, color_value in enumerate(color_values):\n",
    "                key = []\n",
    "                if row_value is not None: key.append(row_value)\n",
    "                if col_value is not None: key.append(col_value)\n",
    "                key.append(color_value)\n",
    "\n",
    "                for metric_index, metric in enumerate(metrics):\n",
    "                    y_values = [results_groupby.loc[tuple(key + [x])][metric] for x in x_values]\n",
    "                    ax.plot(x_values, y_values, marker='o', linestyle='--', linewidth=2, \n",
    "                            color=cmap(color_index * len(metrics) + metric_index), \n",
    "                            label=name_mappings.get(color_value, color_value) if metric_index == 0 else None)\n",
    "\n",
    "            ax.set_xlabel(name_mappings.get(x_key, x_key))\n",
    "            if col_index == 0: ax.set_ylabel(ylabel if ylabel is not None else name_mappings.get(metrics[0], metrics[0]))\n",
    "            ax.set_xticks(x_values)\n",
    "            ax.set_xticklabels(x_values)\n",
    "            if (row_index * n_columns) + col_index  == legend_ax_index: ax.legend()\n",
    "            if column_by_key is not None: ax.set_title(f'{name_mappings.get(column_by_key, column_by_key)}={col_value}')\n",
    "\n",
    "    ylim_min = min(ax.get_ylim()[0] for ax in itertools.chain.from_iterable(axes))\n",
    "    ylim_max = max(ax.get_ylim()[1] for ax in itertools.chain.from_iterable(axes))\n",
    "    for ax in itertools.chain.from_iterable(axes):\n",
    "        ax.set_ylim(ylim_min, ylim_max)\n",
    "\n",
    "    if subplot_adjust_params is not None:\n",
    "        plt.subplots_adjust(**subplot_adjust_params)\n",
    "\n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle, fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='use_lr_scheduler',\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Scheduler')\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='use_lr_scheduler',\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Scheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='batch_size',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Batch Size')\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'patience_epochs', \n",
    "    column_by_key='batch_size',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Batch Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sweep_results(sweep_results_df, 'n_features', 'batch_size', \n",
    "    column_by_key='patience_epochs',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    ylabel='ECDF',\n",
    "    suptitle='ECDF vs. # of Features Used and Patience Epochs')\n",
    "\n",
    "\n",
    "plot_sweep_results(sweep_results_df, 'n_features', 'batch_size', \n",
    "    column_by_key='patience_epochs',\n",
    "    filter_conditions=dict(use_lr_scheduler=False),\n",
    "    subplot_adjust_params=dict(wspace=0.2, hspace=0.25),\n",
    "    metrics=['train_loss', 'test_loss'],\n",
    "    ylabel='Loss',\n",
    "    suptitle='Loss vs. # of Features Used and Patience Epochs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the effect of regrowth depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [c for c in binarized_df.columns if c not in NON_FEATURE_COLUMNS]\n",
    "full_binarized_tensor = utils.df_to_tensor(binarized_df, feature_columns)\n",
    "full_tensor_scores = cv_no_scaling_sq_sq.best_estimator_.transform(full_binarized_tensor).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_diffs = full_tensor_scores[:, 1:] - full_tensor_scores[:, 0].unsqueeze(1)\n",
    "energy_diffs.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_regrowth_depth(game_text: str):\n",
    "    game_id_start = game_text.find('(game')\n",
    "    game_id_section = game_text[game_id_start:game_text.find(')', game_id_start)]\n",
    "    regrowth_depth = game_id_section[game_id_section.rfind('-') + 2:]\n",
    "    return int(regrowth_depth)\n",
    "\n",
    "regrowth_depts = [extract_regrowth_depth(g) for g in regrown_game_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(regrowth_depts, energy_diffs.ravel().numpy(), s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_samples_fitness_df = utils.load_fitness_data('../data/ast_mle_fitness_scores.csv')\n",
    "binarized_map_samples_fitness_df = binarize_features(map_samples_fitness_df)\n",
    "\n",
    "map_samples_binarized_arr = binarized_map_samples_fitness_df.loc[:, [c for c in binarized_map_samples_fitness_df.columns if c not in NON_FEATURE_COLUMNS]]\n",
    "map_samples_binarized_tensor = torch.from_numpy(map_samples_binarized_arr.values).float()\n",
    "\n",
    "map_samples_binarized_energies = cv_no_scaling_sq_sq.best_estimator_.transform(map_samples_binarized_tensor.unsqueeze(1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_log_y = True\n",
    "histogram_title_base = 'Binarized features with MAP games, square-square loss'\n",
    "\n",
    "train_positive_scores = cv_no_scaling_sq_sq.best_estimator_.transform(train_tensor_no_scaling_sq_sq[:, 0, :]).detach().squeeze().numpy()  # type: ignore\n",
    "test_positive_scores = cv_no_scaling_sq_sq.best_estimator_.transform(test_tensor_no_scaling_sq_sq[:, 0, :]).detach().squeeze().numpy()  # type: ignore\n",
    "train_negative_scores = cv_no_scaling_sq_sq.best_estimator_.transform(train_tensor_no_scaling_sq_sq[:, 1:, :]).detach().squeeze().numpy()  # type: ignore\n",
    "test_negative_scores = cv_no_scaling_sq_sq.best_estimator_.transform(test_tensor_no_scaling_sq_sq[:, 1:, :]).detach().squeeze().numpy()  # type: ignore\n",
    "\n",
    "hist_scores = [train_positive_scores, test_positive_scores, \n",
    "               train_negative_scores.flatten(), test_negative_scores.flatten(),\n",
    "               map_samples_binarized_energies.detach().numpy()] \n",
    "\n",
    "labels = ['Real (train)', 'Real (test)', 'Negatives (train)', 'Negatives (test)', 'MAP samples']\n",
    "\n",
    "cm = plt.get_cmap('tab20')  # type: ignore\n",
    "colors = cm.colors[:5]\n",
    "\n",
    "plt.hist(hist_scores, label=labels, stacked=True, bins=100, color=colors)  # type: ignore\n",
    "plt.title(histogram_title_base)\n",
    "\n",
    "plt.xlabel('Energy score')\n",
    "\n",
    "if histogram_log_y:\n",
    "    plt.ylabel('log(Count)')\n",
    "    plt.semilogy()\n",
    "else:\n",
    "    plt.ylabel('Count')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_threshold = 1\n",
    "weights = cv_no_scaling_sq_sq.best_estimator_.named_steps['fitness'].model.fc1.weight.data.detach().squeeze()  # type: ignore\n",
    "weights_above_threshold = (weights.abs() > weight_threshold).numpy()\n",
    "feature_columns = [c for c in binarized_map_samples_fitness_df.columns if c not in NON_FEATURE_COLUMNS]\n",
    "features_with_weight_above_threshold = [feature_columns[i] for i in range(len(feature_columns)) if weights_above_threshold[i]]\n",
    "\n",
    "with open('../data/features_with_weight_above_threshold_2022_01_24.txt', 'w') as f:\n",
    "    f.write('\\n'.join(features_with_weight_above_threshold))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_binarized_tensor = utils.df_to_tensor(binarized_df, [c for c in binarized_df.columns if c not in NON_FEATURE_COLUMNS])\n",
    "test_negative_scores_tensor = torch.tensor(test_negative_scores)\n",
    "test_positive_scores_tensor = torch.tensor(test_positive_scores)\n",
    "feature_columns = [c for c in binarized_df.columns if c not in NON_FEATURE_COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk((test_negative_scores_tensor - test_positive_scores_tensor.unsqueeze(-1)).ravel(), 30, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_negative_scores_tensor.ravel()[836] == test_negative_scores_tensor[836 // 64, 836 % 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_df[(fitness_df.real == 1) & (fitness_df.all_variables_used == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from src.fitness_features import *\n",
    "# from src.fitness_ngram_models import TextNGramModel, TextMultiNGramModel, ASTMultiNGramModel, NGramASTParser\n",
    "\n",
    "# with gzip.open('../models/fitness_featurizer_2023_02_02.pkl.gz', 'rb') as f:\n",
    "#     featurizer = pickle.load(f)\n",
    "from src.evolutionary_sampler_behavioral_features import build_behavioral_features_featurizer, BASIC_BINNED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'raise', 'over': 'raise', 'under': 'raise', 'invalid': 'raise'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    no_binarize=False, \n",
    "    no_merge=False, \n",
    "    use_specific_objects_ngram_model=False,\n",
    "    include_predicate_under_modal_terms=False,\n",
    "    include_arg_types_terms=False,\n",
    "    include_compositionality_terms=False,\n",
    ")\n",
    "featurizer = build_fitness_featurizer(args)\n",
    "np.seterr(all='raise')\n",
    "# featurizer = build_behavioral_features_featurizer(BASIC_BINNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [featurizer.parse(game_asts[i], 'interactive-beta.pddl', return_row=False) for i in range(len(game_asts))]\n",
    "# _ = [featurizer.parse(game_asts[74], 'interactive-beta.pddl', return_row=False) for _ in range(1000)]\n",
    "# _ = [featurizer.parse(regrown_game_asts[i], return_row=False) for i in tqdmn.trange(len(regrown_game_asts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = featurizer.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_file</th>\n",
       "      <th>game_name</th>\n",
       "      <th>domain_name</th>\n",
       "      <th>real</th>\n",
       "      <th>variables_defined_all</th>\n",
       "      <th>variables_defined_prop</th>\n",
       "      <th>variables_used_all</th>\n",
       "      <th>variables_used_prop</th>\n",
       "      <th>preferences_used_all</th>\n",
       "      <th>preferences_used_prop</th>\n",
       "      <th>...</th>\n",
       "      <th>ast_ngram_constraints_n_4_score</th>\n",
       "      <th>ast_ngram_constraints_n_5_score</th>\n",
       "      <th>ast_ngram_terminal_n_2_score</th>\n",
       "      <th>ast_ngram_terminal_n_3_score</th>\n",
       "      <th>ast_ngram_terminal_n_4_score</th>\n",
       "      <th>ast_ngram_terminal_n_5_score</th>\n",
       "      <th>ast_ngram_scoring_n_2_score</th>\n",
       "      <th>ast_ngram_scoring_n_3_score</th>\n",
       "      <th>ast_ngram_scoring_n_4_score</th>\n",
       "      <th>ast_ngram_scoring_n_5_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [src_file, game_name, domain_name, real, variables_defined_all, variables_defined_prop, variables_used_all, variables_used_prop, preferences_used_all, preferences_used_prop, num_preferences_defined_1, num_preferences_defined_2, num_preferences_defined_3, num_preferences_defined_4, num_preferences_defined_5, num_preferences_defined_6, setup_objects_used, setup_quantified_objects_used, any_setup_objects_used, predicate_found_in_data_all, predicate_found_in_data_prop, predicate_found_in_data_small_logicals_all, predicate_found_in_data_small_logicals_prop, adjacent_once_found, adjacent_same_modal_found, starts_and_ends_once, once_in_middle_of_pref_found, pref_without_hold_found, length_of_then_modals_1, length_of_then_modals_2, length_of_then_modals_3, length_of_then_modals_4, length_of_then_modals_5, length_of_then_modals_6, length_of_then_modals_7, at_end_found, max_quantification_count_setup_0, max_quantification_count_setup_1, max_quantification_count_setup_2, max_quantification_count_setup_3, max_quantification_count_setup_4, max_quantification_count_setup_5, max_quantification_count_constraints_0, max_quantification_count_constraints_1, max_quantification_count_constraints_2, max_quantification_count_constraints_3, max_quantification_count_constraints_4, max_quantification_count_constraints_5, max_number_variables_types_quantified_types_1, max_number_variables_types_quantified_types_2, max_number_variables_types_quantified_types_3, max_number_variables_types_quantified_types_4, max_number_variables_types_quantified_types_5, max_number_variables_types_quantified_types_6, max_number_variables_types_quantified_types_7, max_number_variables_types_quantified_types_8, max_number_variables_types_quantified_variables_1, max_number_variables_types_quantified_variables_2, max_number_variables_types_quantified_variables_3, max_number_variables_types_quantified_variables_4, max_number_variables_types_quantified_variables_5, max_number_variables_types_quantified_variables_6, max_number_variables_types_quantified_variables_7, max_number_variables_types_quantified_variables_8, repeated_variables_found, repeated_variable_type_in_either, nested_logicals_found, identical_logical_children_found, identical_scoring_children_found, scoring_count_expression_repetitions_exist, tautological_expression_found, redundant_expression_found, redundant_scoring_terminal_expression_found, unnecessary_expression_found, identical_consecutive_seq_func_predicates_found, disjoint_preferences_found, disjoint_preferences_prop, disjoint_preferences_scoring_terminal_types, disjoint_preferences_scoring_terminal_predicates, disjoint_preferences_same_predicates_only, disjoint_seq_funcs_found, disjoint_modal_predicates_found, disjoint_modal_predicates_prop, predicate_without_variables_or_agent, pref_forall_count_once_per_external_objects_used_correct, pref_forall_count_once_per_external_objects_used_incorrect, pref_forall_count_once_per_external_objects_used_incorrect_count, pref_forall_external_forall_used_correct, pref_forall_external_forall_used_incorrect, pref_forall_external_forall_used_incorrect_count, pref_forall_used_correct, pref_forall_used_incorrect, pref_forall_used_incorrect_count, pref_forall_pref_forall_correct_arity_correct, pref_forall_pref_forall_correct_arity_incorrect, pref_forall_pref_forall_correct_arity_incorrect_count, pref_forall_pref_forall_correct_types_correct, pref_forall_pref_forall_correct_types_incorrect, pref_forall_pref_forall_correct_types_incorrect_count, two_number_operation_found, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 165 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d.predicate_without_variables_or_agent == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = d.mean()\n",
    "m[m == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[(d.disjoint_preferences_scoring_terminal_predicates > 0)][['game_name', 'disjoint_preferences_scoring_terminal_predicates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.predicate_found_in_data_small_logicals_prop != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[[c for c in d.columns if 'in_data' in c]].mean()\n",
    "\n",
    "# print(d.disjoint_modal_predicates_found.mean())\n",
    "# d[d.disjoint_modal_predicates_found == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "_ = [featurizer.parse(game_asts[i], 'interactive-beta.pddl', return_row=True, preprocess_row=False) for i in range(len(game_asts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.disjoint_at_end_found.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unnecessary_expression_found': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\"\"(define (game evo-1912-65-1) (:domain medium-objects-room-v1)\n",
    "(:constraints\n",
    "  (and\n",
    "    (preference preference0\n",
    "      (exists (?v0 ?v1 ?v2 ?v3 - game_object)\n",
    "        (at-end\n",
    "          (or\n",
    "            (in ?v0 ?v2)\n",
    "            (in ?v2 ?v0)\n",
    "         )\n",
    "       )\n",
    "     )\n",
    "   )\n",
    "    (preference preference1\n",
    "      (exists (?v1 - game_object ?v2 - doggie_bed)\n",
    "        (then\n",
    "\n",
    "          (once (and (on ?v2 agent) (agent_holds ?v1)))\n",
    "          (hold (and (not (agent_holds ?v1)) (in_motion ?v1)))\n",
    "          (once (not (in_motion ?v1)))\n",
    "       )\n",
    "     )\n",
    "   )\n",
    " )\n",
    ")\n",
    "(:terminal\n",
    "  (>= (count preference0) 12)\n",
    ")\n",
    "(:scoring\n",
    "  (count preference1)\n",
    ")\n",
    ")\"\"\"\n",
    "test_game = grammar_parser.parse(s)\n",
    "f = featurizer.parse(test_game, 'interactive-beta.pddl', preprocess_row=False, return_row=True)\n",
    "{k: v for k, v in f.items() if 'unnecessary_expression_found' in k}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [2.0, 3.0, 4.0, 10]\n",
    "right = True\n",
    "bins = np.digitize(d.max_width_constraints, thresholds, right)\n",
    "np.unique(bins, return_counts=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[['node_count', 'unique_objects_referenced', 'unique_predicates_referenced']].quantile(np.linspace(0.1, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.plot(x='unique_objects_referenced', y='unique_predicates_referenced', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.plot(y='node_count', kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.assign(original_game_name=d.game_name)  # real=fitness_df.src_file == 'interactive-beta.pddl',\n",
    "d.original_game_name.where(\n",
    "    d.game_name.apply(lambda s: (s.count('-') <= 1) or (s.startswith('game-id') and s.count('-') >= 2)),\n",
    "    d.original_game_name.apply(lambda s: s[:utils._find_nth(s, '-', 2)]),\n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.redundant_scoring_terminal_expression_found.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 0.02\n",
    "\n",
    "mean_features_by_real = filtered_fitness_df[['real'] + [c for c in filtered_fitness_df.columns if c not in NON_FEATURE_COLUMNS]].groupby('real').mean()\n",
    "feature_diffs = mean_features_by_real.loc[1] - mean_features_by_real.loc[0]\n",
    "abs_diffs = feature_diffs.abs()\n",
    "feature_columns = get_features_by_abs_diff_threshold(abs_diffs, score_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(game_asts)):\n",
    "    row = typing.cast(dict, featurizer.parse(game_asts[i], 'interactive-beta.pddl', return_row=True, preprocess_row=True))\n",
    "    df_row = d[d.game_name == row['game_name']]\n",
    "    for key in row:\n",
    "        if key in df_row and row[key] != df_row[key].values[0]:\n",
    "            print(f'In game {row[\"game_name\"]}, {key} mismatch: {row[key]} != {df_row[key].values[0]}')\n",
    "\n",
    "        elif key not in df_row:\n",
    "            print(f'In game {row[\"game_name\"]}, {key} not in df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.game_name == '6172feb1665491d1efbce164-0']['real'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_sum_features = []\n",
    "positive_mean_features = []\n",
    "\n",
    "for feature in feature_columns:\n",
    "    if feature not in d.columns:\n",
    "        continue\n",
    "\n",
    "    if any(x in feature for x in ('arg_types', 'predicate_under_modal', 'max_number', 'max_quantification', 'compositionality_structure', 'depth', 'node_count')):\n",
    "        continue\n",
    "\n",
    "    if d.loc[d.real == True, feature].sum() == 0:\n",
    "        zero_sum_features.append(feature)\n",
    "    else:\n",
    "        positive_mean_features.append(feature)\n",
    "\n",
    "print(f'Zero sum features: {zero_sum_features}')\n",
    "print(f'Positive sum features: {positive_mean_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[[c for c in d.columns if 'max_q' in c]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_game_name in d.original_game_name.unique():\n",
    "    original_game_scoring_score = d[d.game_name == original_game_name].ast_ngram_scoring_n_5_score.max()\n",
    "    sub_frame = d.loc[(d.original_game_name == original_game_name) & (d.real == 0) & (d.ast_ngram_scoring_n_5_score > original_game_scoring_score), 'ast_ngram_scoring_n_5_score']\n",
    "    if len(sub_frame) > 0:\n",
    "        print(original_game_name)\n",
    "        print(sub_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby('real').ast_ngram_scoring_n_5_score.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.section_exists_setup == 0].ast_ngram_setup_n_5_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la /tmp/gd1279/fitness_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = featurizer.to_df()\n",
    "temp_df = pd.read_csv('/tmp/gd1279/fitness_features/fitness_features_1024_regrowths.csv.gz_0.temp.csv')\n",
    "temp_df.columns = d.columns\n",
    "temp_df = utils._add_original_game_name_column(temp_df)\n",
    "d = utils._add_original_game_name_column(d)\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_game_name in d.original_game_name.unique()[1:]:\n",
    "    original_game_scoring_score = d[d.game_name == original_game_name].ast_ngram_scoring_n_5_score.max()\n",
    "    if original_game_scoring_score == 0:\n",
    "        print(original_game_name)\n",
    "        sub_frame = temp_df.loc[(temp_df.original_game_name == original_game_name) & (temp_df.real == 0) & (temp_df.ast_ngram_scoring_n_5_score > original_game_scoring_score),\n",
    "                                ['game_name', 'original_game_name', 'ast_ngram_scoring_n_5_score']]\n",
    "        if len(sub_frame) > 0:\n",
    "            print(original_game_name, len(sub_frame))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_index = 65\n",
    "negative_index = 470\n",
    "\n",
    "pos_r = featurizer.parse(game_asts[game_index], 'test.pddl', return_row=True, preprocess_row=False)\n",
    "neg_r = featurizer.parse(regrown_game_asts[game_index * 1024 + negative_index], 'test.pddl', return_row=True, preprocess_row=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/ast_7_ngram_model_2023_03_06.pkl', 'rb') as f:\n",
    "    ngram_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast_parser import ASTParentMapper\n",
    "parent_mapper = ASTParentMapper()\n",
    "parent_mapper(game_asts[game_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = featurizer.parse(game_asts[17], 'test.pddl', return_row=True, preprocess_row=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in r.items() if k.startswith('section_without')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d207b42274502bc006609ff0f580407f35ab20e7889cda7ddd92e73aeb06c569"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
