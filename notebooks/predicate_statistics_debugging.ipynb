{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 00:13:48 - ast_utils - DEBUG    - Using cache folder: /Users/guydavidson/tmp/game_generation_cache\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import difflib\n",
    "import duckdb\n",
    "from functools import reduce\n",
    "import glob\n",
    "import gzip\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import typing\n",
    "\n",
    "from IPython.display import display, Markdown, HTML  # type: ignore\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import tatsu, tatsu.ast\n",
    "import tqdm.notebook as tqdmn\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('../reward-machine'))\n",
    "\n",
    "import compile_predicate_statistics\n",
    "import compile_predicate_statistics_split_args\n",
    "from compile_predicate_statistics_split_args import *\n",
    "from config import SPECIFIC_NAMED_OBJECTS_BY_ROOM\n",
    "import config\n",
    "from utils import get_object_assignments\n",
    "\n",
    "import ast_printer\n",
    "import ast_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = compile_predicate_statistics.get_project_dir() + '/reward-machine/caches'\n",
    "\n",
    "bitstings_df_path = os.path.join(cache_dir, 'predicate_statistics_bitstring_intervals_028b3733.pkl.gz')\n",
    "bitstrings_df = pd.read_pickle(bitstings_df_path)\n",
    "print(bitstrings_df.shape)\n",
    "with gzip.open(os.path.join(cache_dir, 'trace_lengths_028b3733.pkl'), 'rb') as f:\n",
    "    trace_lengths_and_domains = pickle.load(f)\n",
    "\n",
    "\n",
    "# regular_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics.pkl'))\n",
    "split_args_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics_028b3733.pkl.gz'))\n",
    "# split_args_df = split_args_df[split_args_df['predicate'] != 'same_type']\n",
    "# print(split_args_df.shape)\n",
    "# # split_args_df = pd.read_pickle(os.path.join(cache_dir, 'predicate_statistics_4d5dd602.pkl.gz'))\n",
    "\n",
    "# # stats = compile_predicate_statistics.CommonSensePredicateStatistics(cache_dir)\n",
    "# split_args_stats = compile_predicate_statistics_split_args.CommonSensePredicateStatisticsSplitArgs(\n",
    "#     # cache_dir, compile_predicate_statistics_split_args.CURRENT_TEST_TRACE_NAMES, overwrite=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_index = bitstrings_df[bitstrings_df.arg_1_type.isin(config.ON_EXCLUDED_OBJECT_TYPES) & (bitstrings_df.predicate == 'on')].index\n",
    "# filtered_bitstrings_df = bitstrings_df.drop(drop_index)\n",
    "# filtered_bitstrings_df = filtered_bitstrings_df.reset_index(drop=True)\n",
    "\n",
    "# print(filtered_bitstrings_df.shape, bitstrings_df.shape, filtered_bitstrings_df.index.max())\n",
    "\n",
    "# filtered_bitstrings_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitstrings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game_object_excluded_types = set(config.GAME_OBJECT_EXCLUDED_TYPES)\n",
    "# bitstrings_df = bitstrings_df.assign(arg_1_is_game_object=~bitstrings_df.arg_1_type.isin(game_object_excluded_types), arg_2_is_game_object=~bitstrings_df.arg_2_type.isin(game_object_excluded_types))\n",
    "# bitstrings_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bitstrings_df = bitstrings_df.assign(arg_1_is_block=bitstrings_df.arg_1_type.str.contains('block'), arg_2_is_block=bitstrings_df.arg_2_type.str.contains('block'))\n",
    "bitstrings_df.to_pickle(bitstings_df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bytes([0, 1])\n",
    "BYTE_MAPPING = {b: str(i) for i, b in enumerate(b)}\n",
    "\n",
    "\n",
    "def row_to_string_intervals(row):\n",
    "    value = np.zeros(row['trace_length'], dtype=np.uint8)\n",
    "    for interval in row['intervals']:\n",
    "        value[interval[0]:interval[1]] = 1\n",
    "\n",
    "    return ''.join(map(lambda b: BYTE_MAPPING[b], value.tobytes()))\n",
    "    \n",
    "\n",
    "def create_bitstings_df(df, trace_lengths_and_domains_dict, output_path):\n",
    "    trace_lengths_and_domains_rows = [(key, *value) for key, value in trace_lengths_and_domains_dict.items()]\n",
    "    trace_lengths_and_domains_df = pd.DataFrame(trace_lengths_and_domains_rows, columns=['trace_id', 'trace_length', 'domain'])\n",
    "\n",
    "    split_args_with_trace_length_df = df.join(trace_lengths_and_domains_df.drop(columns=['domain']).set_index('trace_id'), on='trace_id')\n",
    "    split_args_with_string_intervals_df = split_args_with_trace_length_df.assign(intervals=split_args_with_trace_length_df.apply(row_to_string_intervals, axis=1))\n",
    "    split_args_with_string_intervals_df.drop(columns=['trace_length']).to_pickle(output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_lengths_and_domains_rows = [(key, *value) for key, value in trace_lengths_and_domains.items()]\n",
    "trace_lengths_and_domains_df = pd.DataFrame(trace_lengths_and_domains_rows, columns=['trace_id', 'trace_length', 'domain'])\n",
    "\n",
    "split_args_with_trace_length_df = split_args_df.join(trace_lengths_and_domains_df.drop(columns=['domain']).set_index('trace_id'), on='trace_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bytes([0, 1])\n",
    "BYTE_MAPPING = {b: str(i) for i, b in enumerate(b)}\n",
    "\n",
    "\n",
    "def row_to_string_intervals(row):\n",
    "    value = np.zeros(row['trace_length'], dtype=np.uint8)\n",
    "    for interval in row['intervals']:\n",
    "        value[interval[0]:interval[1]] = 1\n",
    "\n",
    "    return ''.join(map(lambda b: BYTE_MAPPING[b], value.tobytes()))\n",
    "    # return np.array2string(value, separator='', threshold=max_length + 10)[1:-1].replace('\\n ', '')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_with_string_intervals_df = split_args_with_trace_length_df.assign(intervals=split_args_with_trace_length_df.apply(row_to_string_intervals, axis=1))\n",
    "split_args_with_string_intervals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_with_string_intervals_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_with_string_intervals_df.loc[\n",
    "    (split_args_with_string_intervals_df.arg_1_type == 'ball'), \"arg_1_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arg_id, arg_type in compile_predicate_statistics_split_args.OBJECT_ID_TYPE_REMAP.items():\n",
    "    original_type = None\n",
    "    if 'window' in arg_id.lower():\n",
    "        original_type = 'window'\n",
    "    if 'shelf' in arg_id.lower():\n",
    "        original_type = 'shelf'\n",
    "    \n",
    "    if original_type is None:\n",
    "        raise ValueError(f'Could not find original type for {arg_id}')\n",
    "    \n",
    "    split_args_with_string_intervals_df.loc[\n",
    "    (split_args_with_string_intervals_df.arg_1_id == arg_id) & (split_args_with_string_intervals_df.arg_1_type == original_type), \"arg_1_type\"] = arg_type\n",
    "\n",
    "    split_args_with_string_intervals_df.loc[\n",
    "    (split_args_with_string_intervals_df.arg_2_id == arg_id) & (split_args_with_string_intervals_df.arg_2_type == original_type), \"arg_2_type\"] = arg_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from config import OBJECTS_BY_ROOM_AND_TYPE, SPECIFIC_NAMED_OBJECTS_BY_ROOM, META_TYPES, GAME_OBJECT, BUILDING\n",
    "\n",
    "all_df_types = set(split_args_with_string_intervals_df.arg_1_type.unique()) | set(split_args_with_string_intervals_df.arg_2_type.unique())\n",
    "computed_types = set(reduce(lambda x, y: x + y, [list(x.keys()) for x in chain(OBJECTS_BY_ROOM_AND_TYPE.values(), SPECIFIC_NAMED_OBJECTS_BY_ROOM.values())]))\n",
    "computed_types.difference_update(META_TYPES.keys())\n",
    "computed_types.remove(GAME_OBJECT)\n",
    "computed_types.add(BUILDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_types - computed_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_types - all_df_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENAMED_TYPES = \"\"\"blue_cube_block\n",
    "tan_cube_block\n",
    "yellow_cube_block\n",
    "blue_pyramid_block\n",
    "red_pyramid_block\n",
    "yellow_pyramid_block\n",
    "blue_dodgeball\n",
    "red_dodgeball\n",
    "pink_dodgeball\n",
    "green_golfball\n",
    "green_triangular_ramp\"\"\".split('\\n')\n",
    "\n",
    "class DefaultValueDict(dict):\n",
    "    def __init__(self, *args, **kawags):\n",
    "        super().__init__(*args, **kawags)\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        return key\n",
    "    \n",
    "arg_type_mapping = DefaultValueDict()\n",
    "for renamed_type in RENAMED_TYPES:\n",
    "    sp = renamed_type.split('_')\n",
    "    new_name = '_'.join(sp[1:] + sp[:1])\n",
    "    arg_type_mapping[renamed_type] =  new_name\n",
    "\n",
    "\n",
    "\n",
    "bitstrings_df.assign(arg_1_type=bitstrings_df.arg_1_type.map(arg_type_mapping), \n",
    "                     arg_2_type=bitstrings_df.arg_2_type.map(arg_type_mapping),).to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitstrings_df.replace(dict(domain=dict(few=0, medium=1, many=2))).to_pickle('predicate_statistics_modified_bitstring_intervals_028b3733.pkl.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_stats.trace_lengths_and_domains_df.select(pl.col('trace_length').max()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(10000, dtype=np.uint8)\n",
    "a[100:1000] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros(10, dtype=np.uint8)\n",
    "t[1:4] = 1\n",
    "b = t.tobytes()\n",
    "d = {b[0]: '0', b[1]: '1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(map(lambda x: d[x], b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bytes([0, 1])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = split_args_stats.trace_lengths_and_domains_df.select(pl.col('trace_length').max()).item()\n",
    "print(MAX_LENGTH)\n",
    "b = bytes([0, 1])\n",
    "BYTE_MAPPING = {b: str(i) for i, b in enumerate(b)}\n",
    "\n",
    "\n",
    "\n",
    "def intervals_to_string(intervals, max_length: int = MAX_LENGTH):\n",
    "    value = np.zeros(max_length, dtype=np.uint8)\n",
    "    for interval in intervals:\n",
    "        value[interval[0]:interval[1]] = 1\n",
    "\n",
    "    return ''.join(map(lambda b: BYTE_MAPPING[b], value.tobytes()))\n",
    "    # return np.array2string(value, separator='', threshold=max_length + 10)[1:-1].replace('\\n ', '')\n",
    "\n",
    "\n",
    "intervals = split_args_df.intervals.apply(intervals_to_string)\n",
    "\n",
    "# small_split_args_df = small_split_args_df.assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_args_df = split_args_df.assign(string_intervals=intervals)\n",
    "split_args_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compile_predicate_statistics_full_database\n",
    "\n",
    "stats = compile_predicate_statistics_full_database.CommonSensePredicateStatisticsFullDatabase(force_trace_names_hash='028b3733')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ast_utils import cached_load_and_parse_games_from_file\n",
    "grammar = open('../dsl/dsl.ebnf').read()\n",
    "grammar_parser = tatsu.compile(grammar)\n",
    "game_asts = list(cached_load_and_parse_games_from_file('../dsl/interactive-beta.pddl', grammar_parser, False, relative_path='.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_game_str = \"\"\"\n",
    "(define (game test-game) (:domain many-objects-room-v1)\n",
    "(:constraints (and\n",
    "    (preference testPred\n",
    "        (exists (?v1 - building ?v2 - block)\n",
    "            (at-end (and\n",
    "                (in ?v2 ?v1)\n",
    "            ))\n",
    "        )\n",
    "    )\n",
    "))\n",
    "(:scoring (+\n",
    "    (* (count testPred) 1)\n",
    ")))\n",
    "\"\"\".strip()\n",
    "# test_game_str = \"\"\"\n",
    "# (define (game test-game) (:domain many-objects-room-v1)\n",
    "# (:constraints (and\n",
    "#     (preference testPred\n",
    "#         (exists (?v0 - block ?v1 - wall)\n",
    "#             (at-end (and \n",
    "#                 (on ?v1 ?v0)\n",
    "#                 (in ?v0 ?v1)\n",
    "#             ))\n",
    "#         )\n",
    "#     )\n",
    "# ))\n",
    "# (:scoring (+\n",
    "#     (* (count testPred) 1)\n",
    "# )))\n",
    "# \"\"\".strip()\n",
    "\n",
    "test_ast = grammar_parser.parse(test_game_str)\n",
    "test_pred = test_ast[3][1].preferences[0].definition.pref_body.body.exists_args.at_end_pred.pred.and_args[0]\n",
    "print(ast_printer.ast_section_to_string(test_pred, ast_parser.PREFERENCES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.filter(test_pred, {'?v1': ['game_object'], '?v2': ['building']})\n",
    "# df = stats.filter(test_pred, {'?v1': ['pillow'], '?v2': ['hexagonal_bin']}, last_interval_bit_set=True)\n",
    "# df = stats.filter(test_pred, {'?v1': ['building'], '?v2': ['flat_block'], '?v3': ['block']}, last_interval_bit_set=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"\"\"\n",
    "WITH t149 AS (WITH t146 AS (SELECT trace_length_and_domains.trace_id as trace_id, trace_length_and_domains.domain as domain, empty_bitstrings.intervals as intervals, object_assignments.\"?v4\" as \"?v4\", object_assignments.\"?v6\" as \"?v6\", object_assignments.\"?v2\" as \"?v2\", object_assignments.\"?v5\" as \"?v5\" FROM trace_length_and_domains JOIN (WITH t142 AS (SELECT domain, object_id AS \"?v4\" FROM object_type_to_id WHERE type NOT IN ('upright'::arg_type, 'yellow'::arg_type, 'wall'::arg_type, 'mirror'::arg_type, 'diagonal'::arg_type, 'front'::arg_type, 'rug'::arg_type, 'bed'::arg_type, 'desktop'::arg_type, 'back'::arg_type, 'pink'::arg_type, 'white'::arg_type, 'red'::arg_type, 'floor'::arg_type, 'poster'::arg_type, 'bottom_shelf'::arg_type, 'green'::arg_type, 'sideways'::arg_type, 'left'::arg_type, 'south_wall'::arg_type, 'blinds'::arg_type, 'west_wall'::arg_type, 'tan'::arg_type, 'agent'::arg_type, 'brown'::arg_type, 'room_center'::arg_type, 'top_shelf'::arg_type, 'main_light_switch'::arg_type, 'east_wall'::arg_type, 'orange'::arg_type, 'right'::arg_type, 'building'::arg_type, 'gray'::arg_type, 'shelf'::arg_type, 'upside_down'::arg_type, 'west_sliding_door'::arg_type, 'north_wall'::arg_type, 'purple'::arg_type, 'shelf_desk'::arg_type, 'east_sliding_door'::arg_type, 'door'::arg_type, 'side_table'::arg_type, 'sliding_door'::arg_type, 'blue'::arg_type, 'desk'::arg_type)), t143 AS (SELECT domain, object_id AS \"?v6\" FROM object_type_to_id WHERE type = 'cube_block_blue'), t144 AS (SELECT domain, object_id AS \"?v2\" FROM object_type_to_id WHERE type NOT IN ('upright'::arg_type, 'yellow'::arg_type, 'wall'::arg_type, 'mirror'::arg_type, 'diagonal'::arg_type, 'front'::arg_type, 'rug'::arg_type, 'bed'::arg_type, 'desktop'::arg_type, 'back'::arg_type, 'pink'::arg_type, 'white'::arg_type, 'red'::arg_type, 'floor'::arg_type, 'poster'::arg_type, 'bottom_shelf'::arg_type, 'green'::arg_type, 'sideways'::arg_type, 'left'::arg_type, 'south_wall'::arg_type, 'blinds'::arg_type, 'west_wall'::arg_type, 'tan'::arg_type, 'agent'::arg_type, 'brown'::arg_type, 'room_center'::arg_type, 'top_shelf'::arg_type, 'main_light_switch'::arg_type, 'east_wall'::arg_type, 'orange'::arg_type, 'right'::arg_type, 'building'::arg_type, 'gray'::arg_type, 'shelf'::arg_type, 'upside_down'::arg_type, 'west_sliding_door'::arg_type, 'north_wall'::arg_type, 'purple'::arg_type, 'shelf_desk'::arg_type, 'east_sliding_door'::arg_type, 'door'::arg_type, 'side_table'::arg_type, 'sliding_door'::arg_type, 'blue'::arg_type, 'desk'::arg_type)), t145 AS (SELECT domain, object_id AS \"?v5\" FROM object_type_to_id WHERE type IN ('bridge_block'::arg_type, 'cube_block'::arg_type, 'cylindrical_block'::arg_type, 'flat_block'::arg_type, 'pyramid_block'::arg_type, 'tall_cylindrical_block'::arg_type, 'tall_rectangular_block'::arg_type, 'triangle_block'::arg_type))\n",
    "SELECT t142.domain, t142.\"?v4\" AS \"?v4\", t143.\"?v6\" AS \"?v6\", t144.\"?v2\" AS \"?v2\", t145.\"?v5\" AS \"?v5\" FROM t142\n",
    "JOIN t143 ON (t142.domain = t143.domain) AND (t142.\"?v4\" != t143.\"?v6\") JOIN t144 ON (t142.domain = t144.domain) AND (t142.\"?v4\" != t144.\"?v2\") AND (t143.\"?v6\" != t144.\"?v2\") JOIN t145 ON (t142.domain = t145.domain) AND (t142.\"?v4\" != t145.\"?v5\") AND (t143.\"?v6\" != t145.\"?v5\") AND (t144.\"?v2\" != t145.\"?v5\")\n",
    ") AS object_assignments ON (trace_length_and_domains.domain = object_assignments.domain) JOIN empty_bitstrings ON (trace_length_and_domains.trace_id = empty_bitstrings.trace_id)), t147 AS (SELECT trace_id, domain, intervals, arg_1_id AS \"?v6\", arg_2_id AS \"?v2\" FROM data WHERE predicate='above' AND (arg_1_type='cube_block_blue') AND arg_2_is_game_object IS TRUE), t148 AS (WITH t141 AS (WITH t139 AS (SELECT trace_length_and_domains.trace_id as trace_id, trace_length_and_domains.domain as domain, empty_bitstrings.intervals as intervals, object_assignments.\"?v4\" as \"?v4\", object_assignments.\"?v5\" as \"?v5\" FROM trace_length_and_domains JOIN (WITH t137 AS (SELECT domain, object_id AS \"?v4\" FROM object_type_to_id WHERE type NOT IN ('upright'::arg_type, 'yellow'::arg_type, 'wall'::arg_type, 'mirror'::arg_type, 'diagonal'::arg_type, 'front'::arg_type, 'rug'::arg_type, 'bed'::arg_type, 'desktop'::arg_type, 'back'::arg_type, 'pink'::arg_type, 'white'::arg_type, 'red'::arg_type, 'floor'::arg_type, 'poster'::arg_type, 'bottom_shelf'::arg_type, 'green'::arg_type, 'sideways'::arg_type, 'left'::arg_type, 'south_wall'::arg_type, 'blinds'::arg_type, 'west_wall'::arg_type, 'tan'::arg_type, 'agent'::arg_type, 'brown'::arg_type, 'room_center'::arg_type, 'top_shelf'::arg_type, 'main_light_switch'::arg_type, 'east_wall'::arg_type, 'orange'::arg_type, 'right'::arg_type, 'building'::arg_type, 'gray'::arg_type, 'shelf'::arg_type, 'upside_down'::arg_type, 'west_sliding_door'::arg_type, 'north_wall'::arg_type, 'purple'::arg_type, 'shelf_desk'::arg_type, 'east_sliding_door'::arg_type, 'door'::arg_type, 'side_table'::arg_type, 'sliding_door'::arg_type, 'blue'::arg_type, 'desk'::arg_type)), t138 AS (SELECT domain, object_id AS \"?v5\" FROM object_type_to_id WHERE type IN ('bridge_block'::arg_type, 'cube_block'::arg_type, 'cylindrical_block'::arg_type, 'flat_block'::arg_type, 'pyramid_block'::arg_type, 'tall_cylindrical_block'::arg_type, 'tall_rectangular_block'::arg_type, 'triangle_block'::arg_type))\n",
    "SELECT t137.domain, t137.\"?v4\" AS \"?v4\", t138.\"?v5\" AS \"?v5\" FROM t137\n",
    "JOIN t138 ON (t137.domain = t138.domain) AND (t137.\"?v4\" != t138.\"?v5\")\n",
    ") AS object_assignments ON (trace_length_and_domains.domain = object_assignments.domain) JOIN empty_bitstrings ON (trace_length_and_domains.trace_id = empty_bitstrings.trace_id)), t140 AS (SELECT trace_id, domain, intervals, arg_1_id AS \"?v4\", arg_2_id AS \"?v5\" FROM data WHERE predicate='adjacent' AND arg_1_is_game_object IS TRUE AND arg_2_is_block IS TRUE)\n",
    "        SELECT t139.trace_id as trace_id, t139.domain as domain, t139.\"?v4\" as \"?v4\", t139.\"?v5\" as \"?v5\", (~( t139.intervals | COALESCE(t140.intervals, t139.intervals) )) AS intervals FROM t139 LEFT JOIN t140 ON t139.\"trace_id\"=t140.\"trace_id\" AND t139.\"?v4\"=t140.\"?v4\" AND t139.\"?v5\"=t140.\"?v5\"\n",
    "        ) SELECT * FROM t141 WHERE bit_count(intervals) != 0) SELECT t146.trace_id, t146.domain, t146.\"?v4\", t146.\"?v6\", t146.\"?v2\", t146.\"?v5\", (t146.intervals | COALESCE(t147.intervals, t146.intervals) | COALESCE(t148.intervals, t146.intervals)) AS intervals FROM t146 LEFT JOIN t147 ON (t146.trace_id=t147.trace_id) AND (t146.\"?v6\"=t147.\"?v6\") AND (t146.\"?v2\"=t147.\"?v2\") LEFT JOIN t148 ON (t146.trace_id=t148.trace_id) AND (t146.\"?v4\"=t148.\"?v4\") AND (t146.\"?v5\"=t148.\"?v5\")) SELECT * FROM t149 WHERE bit_count(intervals) != 0\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT trace_id, arg_1_id, arg_2_id, overlap, d1_count, d2_count\n",
    "FROM (WITH \n",
    "d1 AS (SELECT trace_id, arg_1_id, arg_1_type, arg_2_id, arg_2_type, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building'), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building')\n",
    "SELECT d1.trace_id, d1.arg_1_id, d1.arg_1_type, d1.arg_2_id, d1.arg_2_type, bit_count(d1.intervals & d2.intervals) as overlap, bit_count(d1.intervals) as d1_count, bit_count(d2.intervals) as d2_count\n",
    "FROM d1\n",
    "INNER JOIN d2 ON d1.trace_id = d2.trace_id AND d1.arg_1_id = d2.arg_2_id AND d1.arg_2_id = d2.arg_1_id)\n",
    "WHERE overlap > 0\n",
    "\"\"\"\n",
    "reciprocal_on_df = duckdb.sql(q).df()  # .to_csv('temp_outputs/a_on_b_and_b_on_a.csv')\n",
    "reciprocal_on_df\n",
    "# reciprocal_on_df = reciprocal_on_df.assign(remove_d1=reciprocal_on_df.d1_count <= reciprocal_on_df.d2_count)\n",
    "# reciprocal_on_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('CREATE INDEX idx_data_arg_1_type ON data (arg_1_type)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicate_on_rows = bitstrings_df.predicate == 'on'\n",
    "indices_to_remove = []\n",
    "\n",
    "for _, row in reciprocal_on_df.iterrows():\n",
    "    row_filter = predicate_on_rows & (bitstrings_df.trace_id == row.trace_id)\n",
    "    if row.remove_d1:\n",
    "        row_filter &= (bitstrings_df.arg_1_id == row.arg_1_id) & (bitstrings_df.arg_2_id == row.arg_2_id)\n",
    "\n",
    "    else:\n",
    "        row_filter &= (bitstrings_df.arg_1_id == row.arg_2_id) & (bitstrings_df.arg_2_id == row.arg_1_id)\n",
    "\n",
    "    indices_to_remove.extend(bitstrings_df[row_filter].index)\n",
    "\n",
    "\n",
    "print(indices_to_remove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(indices_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_bitstrings_df = bitstrings_df.drop(index=indices_to_remove)\n",
    "# filtered_bitstrings_df = filtered_bitstrings_df.reset_index(drop=True)\n",
    "\n",
    "# print(filtered_bitstrings_df.shape, bitstrings_df.shape, filtered_bitstrings_df.index.max())\n",
    "\n",
    "# filtered_bitstrings_df.to_pickle(bitstings_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT trace_id, arg_1_id, arg_2_id, overlap  \n",
    "FROM (WITH \n",
    "d1 AS (SELECT trace_id, arg_1_id, arg_1_type, intervals FROM data WHERE predicate='agent_holds'), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='adjacent' AND (arg_1_type='agent' OR arg_2_type='agent'))\n",
    "SELECT d2.trace_id, d2.arg_1_id, d2.arg_2_id, bit_count(d1.intervals & d2.intervals) as overlap\n",
    "FROM d2\n",
    "INNER JOIN d1 ON d1.trace_id = d2.trace_id AND (d1.arg_1_id = d2.arg_1_id OR d1.arg_1_id = d2.arg_2_id)\n",
    ")\n",
    "WHERE overlap > 0\n",
    "\"\"\"\n",
    "duckdb.sql(q).df()  # .to_csv('temp_outputs/a_on_b_and_b_on_a.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT count(*) FROM data\n",
    "WHERE predicate='agent_holds'\n",
    "\"\"\"\n",
    "duckdb.sql(q)  # .df()  # .to_csv('temp_outputs/a_on_b_and_b_on_a.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_arg_types = duckdb.sql(\"SELECT arg_types.* FROM (SELECT DISTINCT(arg_1_type, arg_2_type) as arg_types FROM data WHERE predicate='in');\").fetchall()\n",
    "in_arg_types = [tuple(x) for x in in_arg_types]\n",
    "first_arg_types, second_arg_types = zip(*in_arg_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in in_arg_types if t[0] == 'mug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_arg_types = duckdb.sql(\"SELECT arg_types.* FROM (SELECT DISTINCT(arg_1_type, arg_2_type) as arg_types FROM data WHERE predicate='on');\").fetchall()\n",
    "on_arg_types = [tuple(x) for x in on_arg_types]\n",
    "on_first_arg_types, on_second_arg_types = zip(*on_arg_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in on_arg_types if t[1] == 'desk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import room_and_object_types\n",
    "\n",
    "on_types_by_category = defaultdict(set)\n",
    "for t in on_first_arg_types:\n",
    "    on_types_by_category[room_and_object_types.TYPES_TO_CATEGORIES[t]].add(t)\n",
    "\n",
    "\n",
    "for cat in on_types_by_category:\n",
    "    print(cat)\n",
    "    print(on_types_by_category[cat])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT * FROM (WITH \n",
    "d1 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building'), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building')\n",
    "SELECT d1.trace_id, d1.arg_1_id, d1.arg_2_id, bit_count(d1.intervals & d2.intervals) as overlap\n",
    "FROM d1\n",
    "INNER JOIN d2 ON d1.trace_id = d2.trace_id AND d1.arg_1_id = d2.arg_2_id AND d1.arg_2_id = d2.arg_1_id)\n",
    "WHERE overlap > 100\n",
    "\"\"\"\n",
    "duckdb.sql(q).df().to_csv('temp_outputs/a_on_b_and_b_on_a.csv')\n",
    "\n",
    "# duckdb.sql(\"SELECT * FROM data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT * FROM (WITH \n",
    "d1 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND arg_1_type!='building' AND arg_2_type!='building'), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='in' AND arg_1_type!='building' AND arg_2_type!='building')\n",
    "SELECT d1.trace_id, d1.arg_1_id, d1.arg_2_id, bit_count(d1.intervals & d2.intervals) as overlap\n",
    "FROM d1\n",
    "INNER JOIN d2 ON d1.trace_id = d2.trace_id AND d1.arg_1_id = d2.arg_2_id AND d1.arg_2_id = d2.arg_1_id)\n",
    "WHERE overlap > 100\n",
    "\"\"\"\n",
    "duckdb.sql(q).df().to_csv('temp_outputs/a_on_b_and_b_in_a.csv')\n",
    "\n",
    "# duckdb.sql(\"SELECT * FROM data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT trace_id, domain, arg_1_id, arg_2_id, bit_position('1'::BIT, joint_intervals) as \"first_index\" FROM (WITH \n",
    "d1 AS (SELECT trace_id, domain, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND (arg_1_type='building' OR arg_2_type='building')), \n",
    "d2 AS (SELECT trace_id, arg_1_id, arg_2_id, intervals FROM data WHERE predicate='on' AND (arg_1_type='building' OR arg_2_type='building'))\n",
    "SELECT d1.trace_id, d1.domain, d1.arg_1_id, d1.arg_2_id, d1.intervals & d2.intervals as joint_intervals\n",
    "FROM d1\n",
    "INNER JOIN d2 ON d1.trace_id = d2.trace_id AND d1.arg_1_id = d2.arg_2_id AND d1.arg_2_id = d2.arg_1_id)\n",
    "WHERE bit_count(joint_intervals) > 100\n",
    "\"\"\"\n",
    "buildings_df = duckdb.sql(q).df().to_csv('temp_outputs/a_on_b_and_b_on_a_buildings.csv')\n",
    "buildings_df\n",
    "# buildings_df[(buildings_df.trace_id == 'Q6a8AbiIdcLA9tJzAu14-createGame-rerecorded') & (buildings_df.arg_2_id == 'SmallSlide|-00.81|+00.14|-03.10')]\n",
    "\n",
    "# duckdb.sql(\"SELECT * FROM data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT * FROM data\n",
    "WHERE trace_id='Q6a8AbiIdcLA9tJzAu14-createGame-rerecorded' AND arg_1_id='building_1' and arg_2_id='SmallSlide|-00.81|+00.14|-03.10' AND predicate='on'\n",
    "\"\"\"\n",
    "\n",
    "d = duckdb.sql(q).df()\n",
    "print(d.loc[0, 'intervals'] == d.loc[1, 'intervals'])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT trace_id, domain, arg_1_id, arg_2_id, bit_position('1'::BIT, intervals) as 'first_index' FROM data\n",
    "WHERE predicate='on' and arg_2_type in ('bed', 'desk') and arg_1_type NOT IN ('floor', 'rug')\n",
    "\"\"\"\n",
    "duckdb.sql(q).df().to_csv('temp_outputs/bed_or_desk_on_object_that_is_not_floor_or_rug.csv')\n",
    "\n",
    "# duckdb.sql(\"SELECT * FROM data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = game_asts[1][4][1].preferences[0].definition.forall_pref.preferences.pref_body.body.then_funcs[0].seq_func.once_pred\n",
    "print(p.keys())\n",
    "ast_printer.ast_section_to_string(p, ast_parser.PREFERENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = stats.filter(p, {\"?b\": [\"ball\"], \"?t\": [\"hexagonal_bin\"]})\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('PRAGMA force_index_join;')\n",
    "duckdb.sql(\"PRAGMA explain_output='OPTIMIZED_ONLY';\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = \"\"\"\n",
    "#     SELECT t0.trace_id, t0.domain, t0.\"?b\", t1.\"door\", t1.\"agent\", (t0.intervals & t1.intervals) AS intervals\n",
    "#     FROM (SELECT trace_id, domain, intervals, arg_1_id AS \"?b\" FROM data WHERE predicate='agent_holds' AND (arg_1_type IN ('beachball'::arg_type, 'basketball'::arg_type, 'dodgeball'::arg_type, 'golfball'::arg_type))) as t0\n",
    "#     INNER JOIN (SELECT trace_id, domain, intervals, arg_1_id AS \"door\", arg_2_id AS \"agent\" FROM data WHERE predicate='adjacent' AND (arg_1_type='door') AND (arg_2_type='agent')) as t1\n",
    "#     ON (t0.trace_id=t1.trace_id)\n",
    "# \"\"\"\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT t0.trace_id, t0.domain, t0.arg_1_id AS \"?b\", t1.arg_1_id AS \"door\", t1.arg_2_id AS \"agent\", (t0.intervals & t1.intervals) AS intervals\n",
    "    FROM data AS t0\n",
    "    INNER JOIN data AS t1\n",
    "    ON (t0.trace_id=t1.trace_id)\n",
    "    WHERE t0.predicate='agent_holds' AND (t0.arg_1_type IN ('beachball'::arg_type, 'basketball'::arg_type, 'dodgeball'::arg_type, 'golfball'::arg_type)) AND\n",
    "    t1.predicate='adjacent' AND (t1.arg_1_type='door') AND (t1.arg_2_type='agent')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(duckdb.sql(f\"EXPLAIN ANALYZE ({q})\").fetchone()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"INSERT INTO domains VALUES ('few'), ('medium'), ('many')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duckdb.sql(q).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql('CREATE INDEX idx_data_predicate ON data (predicate)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.zeros(10, dtype=np.uint8)\n",
    "t[1:4] = 1\n",
    "b = t.tobytes()\n",
    "d = {b[0]: '0', b[1]: '1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.trace_id == '1El1CmicSoKZKTLe8NpP-preCreateGame-rerecorded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_from_df = np.unpackbits(np.frombuffer(interval_from_df, dtype=np.uint8))\n",
    "len(bits_from_df), bits_from_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = duckdb.sql(q).fetchall()\n",
    "\n",
    "for tup in results:\n",
    "    sub_df = df[(tup[0] == df['trace_id']) & (tup[2] == df['?b']) & (tup[3] == df['?t'])]\n",
    "    if len(sub_df) != 1:\n",
    "        print(f'Error: {tup[:-1]}')\n",
    "        print(len(sub_df))\n",
    "        break\n",
    "\n",
    "    expected_length = duckdb.sql(f\"SELECT length from trace_length_and_domains WHERE trace_id='{tup[0]}'\").fetchone()[0]\n",
    "\n",
    "    bits_from_df = np.unpackbits(np.frombuffer(sub_df.intervals.item(), dtype=np.uint8))\n",
    "    bits_from_db = np.fromiter(map(int, tup[-1]), dtype=np.uint8)\n",
    "\n",
    "    if len(bits_from_db) != expected_length:\n",
    "        print(f'Error: {tup[:-1]}')\n",
    "        print(len(bits_from_db))\n",
    "        print(expected_length)\n",
    "        print(len(bits_from_df))\n",
    "        break\n",
    "\n",
    "    if not np.all(bits_from_df[-expected_length:] == bits_from_db):\n",
    "        print(f'Error: {tup[:-1]}')\n",
    "        print(np.where(bits_from_df[-expected_length:] != bits_from_db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bits_from_df[-550:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_from_db = np.fromiter(map(int, t[-1]), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_from_df = np.unpackbits(np.frombuffer(df[df.trace_id == t[0]].intervals.item(), dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bits_from_db), len(bits_from_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1264 % 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bits_from_db), len(bits_from_df))\n",
    "np.where(bits_from_db[:] != bits_from_df[14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_from_db[500:540], bits_from_df[500:540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bits_from_db[:min_length] != bits_from_df[:min_length])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gameplan from here\n",
    "\n",
    "* For (setup, each preference):\n",
    "* Inspect to make sure all predicates implemeneted in the cache, it doesn't use a once-measure (and probably also not a hold-while, at least at first), etc. \n",
    "    * otherwise default to the basic implementation\n",
    "* If we're go to run on a particular thing:\n",
    "    * If it's a setup, it's trivial\n",
    "    * If it's a preference, check if it's an at-end or then\n",
    "        * If it's an at-end, it's trivial (can probably fold this into the query by doing `get_bit(intervals, length - 1)` if we join on the trace lengths table\n",
    "        * If it's a then, enumerate over the predicates of each modal, and query for them\n",
    "            * For each one, fetch the df for the query, use the trace lengths to transform the intervals to the expected format\n",
    "            * Add the index of the modal to the df\n",
    "            * Enumerate through all trace ids and assignments, and for each assignment where we have all modals represented: \n",
    "                * Create the joint state interval\n",
    "                * Iterate through the joint using the state machine logic\n",
    "                * Count satisfactions\n",
    "                * ...\n",
    "                * Profit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tatsu.ast\n",
    "import tatsu.grammars\n",
    "from ast_parser import ASTParser, SECTION_CONTEXT_KEY, VARIABLES_CONTEXT_KEY\n",
    "from ast_utils import simplified_context_deepcopy, deepcopy_ast, ASTCopyType, replace_child\n",
    "\n",
    "\n",
    "DEFAULT_UNSUPPORTED_RULES = [\n",
    "    'function_comparison',\n",
    "    'function_eval',\n",
    "    'predicate_adjacent_side_3',\n",
    "    'predicate_adjacent_side_4',\n",
    "    'predicate_between',\n",
    "    'predicate_faces',\n",
    "    'predicate_is_setup_object',\n",
    "    'predicate_opposite',\n",
    "    'predicate_rug_color_under',\n",
    "    'predicate_same_color',\n",
    "    'predicate_same_object',\n",
    "    'predicate_same_type',\n",
    "    'super_predicate_exists',\n",
    "    'super_predicate_forall',\n",
    "    'once_measure',\n",
    "    'while_hold',\n",
    "]\n",
    "\n",
    "\n",
    "def _pref_forall_pos_to_key(pos: int):\n",
    "    return f'pref_forall_{pos}'\n",
    "\n",
    "\n",
    "class MixedTraceFilterGameParser(ASTParser):\n",
    "    unsupported_rules: typing.Set[str]\n",
    "\n",
    "    def __init__(self, unsupported_rules: typing.Sequence[str] = DEFAULT_UNSUPPORTED_RULES):\n",
    "        super().__init__()\n",
    "        self.expected_keys = set()\n",
    "        self.unsupported_rules = set(unsupported_rules)\n",
    "\n",
    "    def __call__(self, ast, **kwargs):\n",
    "        initial_call = 'inner_call' not in kwargs or not kwargs['inner_call']\n",
    "        if initial_call:\n",
    "            kwargs['inner_call'] = True\n",
    "            kwargs['local_context'] = {'mapping': {VARIABLES_CONTEXT_KEY: {}}}\n",
    "            kwargs['global_context'] = {}\n",
    "            self.expected_keys = set()\n",
    "            self.unsupported_keys = set()\n",
    "            # self.traces_by_preference_or_section = {}\n",
    "            # self.preferences_or_sections_with_implemented_predicates = set()\n",
    "            # self.predicate_strings_by_preference_or_section = defaultdict(set)\n",
    "            # self.not_implemented_predicate_counts = defaultdict(int)\n",
    "\n",
    "        retval = super().__call__(ast, **kwargs)\n",
    "\n",
    "        if initial_call:\n",
    "            return self.unsupported_keys, self.expected_keys\n",
    "        else:\n",
    "            return retval\n",
    "\n",
    "    def _current_ast_to_contexts_hook(self, ast: tatsu.ast.AST, kwargs: typing.Dict[str, typing.Any]):\n",
    "        rule = typing.cast(str, ast.parseinfo.rule)  # type: ignore\n",
    "\n",
    "        if rule == 'pref_forall':\n",
    "            kwargs['local_context']['current_pref_forall_index'] = ast.parseinfo.pos\n",
    "\n",
    "        if rule == 'preference':\n",
    "            kwargs['local_context']['current_preference_name'] = ast.pref_name\n",
    "\n",
    "    def _handle_ast(self, ast: tatsu.ast.AST, **kwargs):\n",
    "        self._current_ast_to_contexts(ast, **kwargs)\n",
    "        kwargs['local_context']['mapping'] = ast_parser.update_context_variables(ast, kwargs['local_context']['mapping'])\n",
    "\n",
    "        current_key = None\n",
    "        if SECTION_CONTEXT_KEY in kwargs and kwargs[SECTION_CONTEXT_KEY] == ast_parser.SETUP:\n",
    "            current_key = kwargs[SECTION_CONTEXT_KEY]\n",
    "        elif 'current_pref_forall_index' in kwargs['local_context']:\n",
    "            current_key =_pref_forall_pos_to_key(kwargs['local_context']['current_pref_forall_index'])\n",
    "        elif 'current_preference_name' in kwargs['local_context']:\n",
    "            current_key = kwargs['local_context']['current_preference_name']\n",
    "        \n",
    "        if current_key is not None:\n",
    "            self.expected_keys.add(current_key)\n",
    "\n",
    "            if ast.parseinfo.rule in self.unsupported_rules:\n",
    "                self.unsupported_keys.add(current_key)\n",
    "\n",
    "        for key in ast:\n",
    "            if key != 'parseinfo':\n",
    "                child_kwargs = simplified_context_deepcopy(kwargs)\n",
    "                retval = self(ast[key], **child_kwargs)\n",
    "                self._update_contexts_from_retval(kwargs, retval)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_parser = MixedTraceFilterGameParser()\n",
    "for ast in game_asts:\n",
    "    unsupported, expected = game_parser(ast)\n",
    "    supported = expected - unsupported\n",
    "    print(f'Game {ast[1].game_name} has supported keys: {list(supported)} and unsupported keys: {list(unsupported)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "DUMMY_PREFERENCE_GAME = \"\"\"(define (game dummy-preference-game) (:domain many-objects-room-v1)\n",
    "(:constraints (and\n",
    "    (preference dummyPreference\n",
    "            (at-end (game-over))\n",
    "    )\n",
    "))\n",
    "(:scoring (count dummyPreference)\n",
    "))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ASTTraceFilterSplitter(ast_parser.ASTParser):\n",
    "    keep_keys: typing.Set[str]\n",
    "    remove_keys: typing.Set[str]\n",
    "    should_insert_dummy_preference: bool\n",
    "    \n",
    "    def __init__(self, grammar_parser: tatsu.grammars.Grammar):\n",
    "        self.grammar_parser = grammar_parser\n",
    "\n",
    "    def __call__(self, ast, **kwargs):\n",
    "        initial_call = 'inner_call' not in kwargs or not kwargs['inner_call']\n",
    "        if initial_call:\n",
    "            kwargs['inner_call'] = True\n",
    "            \n",
    "            if 'remove_keys' not in kwargs:\n",
    "                raise ValueError('remove_keys must be specified')\n",
    "            self.remove_keys = kwargs['remove_keys']\n",
    "\n",
    "            if len(self.remove_keys) == 0:\n",
    "                raise ValueError('remove_keys must be non-empty')\n",
    "\n",
    "            if 'keep_keys' not in kwargs:\n",
    "                raise ValueError('keep_keys must be specified')\n",
    "            self.keep_keys = kwargs['keep_keys']\n",
    "\n",
    "            if len(self.keep_keys) == 0:\n",
    "                raise ValueError('keep_keys must be non-empty')\n",
    "\n",
    "            ast = deepcopy_ast(ast)\n",
    "\n",
    "            # Handle the setup right here and now, if we're removing it\n",
    "            if ast_parser.SETUP in self.remove_keys:\n",
    "                ast = (*ast[:3], *ast[4:])\n",
    "                # If the only thin we're removing is the setup, we're done\n",
    "                if len(self.remove_keys) == 1:\n",
    "                    return ast\n",
    "\n",
    "            # check if we're only keeping the setup and inserting a dummy preference, because if so, we're done\n",
    "            if len(self.keep_keys) == 1 and ast_parser.SETUP in self.keep_keys:\n",
    "                dummy_preference_game = self.grammar_parser.parse(DUMMY_PREFERENCE_GAME)\n",
    "                return (*ast[:4], dummy_preference_game[3], *ast[4:])\n",
    "\n",
    "        super().__call__(ast, **kwargs)\n",
    "\n",
    "        if initial_call:\n",
    "            return ast\n",
    "        \n",
    "    def _handle_ast(self, ast: tatsu.ast.AST, **kwargs):\n",
    "        rule = ast.parseinfo.rule\n",
    "\n",
    "        if rule == 'preferences':\n",
    "            if isinstance(ast.preferences, tatsu.ast.AST):\n",
    "                raise ValueError(f'If removing a single preference, the initial call should handle it, so this should never occur')\n",
    "            \n",
    "            new_children = typing.cast(typing.List[tatsu.ast.AST], deepcopy_ast(ast.preferences, ASTCopyType.NODE))\n",
    "            indices_to_remove = []\n",
    "            for i, child in enumerate(new_children):\n",
    "                if child.parseinfo.rule == 'preference' and child.pref_name in self.remove_keys:\n",
    "                    print(f'Removing preference {child.pref_name}')\n",
    "                    indices_to_remove.append(i)\n",
    "                elif child.parseinfo.rule == 'pref_forall' and _pref_forall_pos_to_key(child.parseinfo.pos) in self.remove_keys:\n",
    "                    print(f'Removing pref_forall {_pref_forall_pos_to_key(child.parseinfo.pos)}')\n",
    "                    indices_to_remove.append(i)\n",
    "\n",
    "            for i in reversed(indices_to_remove):\n",
    "                new_children.remove(new_children[i])\n",
    "\n",
    "            replace_child(ast, 'preferences', new_children)\n",
    "\n",
    "        else:\n",
    "            for key in ast:\n",
    "                if key != 'parseinfo':\n",
    "                    self(ast[key], **kwargs)\n",
    "\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_parser = MixedTraceFilterGameParser()\n",
    "game_splitter = ASTTraceFilterSplitter(grammar_parser)  # type: ignore\n",
    "ast = game_asts[0]\n",
    "unsupported, expected = game_parser(ast)\n",
    "supported = expected - unsupported\n",
    "\n",
    "if len(supported) > 0 and len(unsupported) > 0:\n",
    "    print(f'Game {ast[1].game_name} has supported keys: {list(supported)} and unsupported keys: {list(unsupported)}')\n",
    "    supported_only = game_splitter(ast, keep_keys=supported, remove_keys=unsupported)\n",
    "    unsupported_only = game_splitter(ast, keep_keys=unsupported, remove_keys=supported)\n",
    "\n",
    "    print('=' * 80)\n",
    "    print(ast_printer.ast_to_string(supported_only, '\\n'))\n",
    "    print('=' * 80)\n",
    "    print(ast_printer.ast_to_string(unsupported_only, '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "categorical_type = pd.api.types.CategoricalDtype(sorted(chain.from_iterable(duckdb.sql(\"SELECT enum_range(NULL::trace_id)\").fetchone())), ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id_to_length_df = duckdb.sql('SELECT * FROM trace_length_and_domains').fetchdf()\n",
    "trace_id_to_length_df.drop(columns=['domain'], inplace=True)\n",
    "trace_id_to_length_df.rename(columns=dict(length='trace_length'), inplace=True)\n",
    "# trace_id_to_length_df.astype(dict(trace_id=categorical_type), copy=False)\n",
    "trace_id_to_length_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_intervals_to_array(row):\n",
    "    return np.unpackbits(np.frombuffer(row['intervals'], dtype=np.uint8))[-row['trace_length']:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype(dict(trace_id=categorical_type), copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.join(trace_id_to_length_df, on=['trace_id'], how='outer', rsuffix='_r')\n",
    "merged_df = df.merge(trace_id_to_length_df, on=['trace_id'], how='left')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_df = df.assign(intervals=merged_df.apply(_df_intervals_to_array, axis=1))\n",
    "assigned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigned_df.groupby('trace_id', as_index=True, observed=False).intervals.transform(lambda l: list(np.logical_or.reduce(*l)))\n",
    "\n",
    "series_1 = assigned_df.groupby('trace_id', as_index=True, observed=True).intervals.agg(lambda x: reduce(np.logical_or, x.values).astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_2 = series_1.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(series_1, series_2, left_index=True, right_index=True, how='inner')\n",
    "s = merged.agg(lambda row: np.logical_and(row['intervals_x'], row['intervals_y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
